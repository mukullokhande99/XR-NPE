{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5zb4il6oaYnS",
        "outputId": "557d9642-1a42-42b4-ee57-bf657b88fc43"
      },
      "outputs": [],
      "source": [
        "\"\"\"FP4 Quantization of Gazelle Model\"\"\"\n",
        "\n",
        "import os\n",
        "import subprocess\n",
        "import sys\n",
        "\n",
        "def execute_shell_command(command):\n",
        "    \"\"\"Run a shell command\"\"\"\n",
        "    try:\n",
        "        subprocess.run(command, shell=True, check=True, text=True)\n",
        "        print(f\"Executed: {command}\")\n",
        "    except subprocess.CalledProcessError as e:\n",
        "        print(f\"Error executing {command}: {e}\")\n",
        "\n",
        "# Clone repo if not present\n",
        "if not os.path.exists('/content/gazelle'):\n",
        "    execute_shell_command(\"git clone https://github.com/fkryan/gazelle.git\")\n",
        "else:\n",
        "    print(\"Gazelle repo already exists\")\n",
        "\n",
        "# Add to Python path and set working directory\n",
        "sys.path.insert(0, '/content/gazelle')\n",
        "os.chdir('/content/gazelle')\n",
        "\n",
        "# Install dependencies\n",
        "execute_shell_command(\"pip install torch torchvision timm matplotlib opencv-python scipy transformers\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IfSJBKR1alSF"
      },
      "outputs": [],
      "source": [
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import math\n",
        "import numpy as np\n",
        "import copy\n",
        "from PIL import Image\n",
        "import matplotlib.pyplot as plt\n",
        "from torchvision import transforms"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JudvpQfQashu"
      },
      "outputs": [],
      "source": [
        "class FP4Quantizer(torch.autograd.Function):\n",
        "    \"\"\"Custom autograd function for FP4 quantization\"\"\"\n",
        "    @staticmethod\n",
        "    def forward(ctx, input_tensor, scale, zero_point):\n",
        "        original_shape = input_tensor.shape\n",
        "        flat_tensor = input_tensor.flatten()\n",
        "        quantized = torch.clamp(torch.round(flat_tensor / (scale + 1e-8)) + zero_point, 0, 15)\n",
        "        dequantized = (quantized - zero_point) * scale\n",
        "        return dequantized.reshape(original_shape)\n",
        "\n",
        "    @staticmethod\n",
        "    def backward(ctx, grad_output):\n",
        "        return grad_output, None, None\n",
        "\n",
        "def calculate_fp4_scale_zero_point(tensor, symmetric=True):\n",
        "    \"\"\"Compute scale and zero point for FP4 quantization\"\"\"\n",
        "    if symmetric:\n",
        "        abs_max = tensor.abs().max()\n",
        "        scale = (abs_max / 7.5) if abs_max > 1e-8 else torch.tensor(1e-3)\n",
        "        zero_point = torch.tensor(7.5)\n",
        "    else:\n",
        "        min_val, max_val = tensor.min(), tensor.max()\n",
        "        range_val = max_val - min_val\n",
        "        scale = (range_val / 15) if range_val > 1e-8 else torch.tensor(1e-3)\n",
        "        zero_point = torch.clamp((-min_val / scale).round(), 0, 15)\n",
        "    return scale.to(tensor.device), zero_point.to(tensor.device)\n",
        "\n",
        "def quantize_tensor_to_fp4(tensor, symmetric=True):\n",
        "    \"\"\"Quantize tensor to FP4 format\"\"\"\n",
        "    scale, zero_point = calculate_fp4_scale_zero_point(tensor, symmetric)\n",
        "    quantized_tensor = FP4Quantizer.apply(tensor, scale, zero_point)\n",
        "    return quantized_tensor, scale, zero_point\n",
        "\n",
        "class FP4LinearLayer(nn.Module):\n",
        "    \"\"\"FP4 quantized linear layer\"\"\"\n",
        "    def __init__(self, original_linear):\n",
        "        super().__init__()\n",
        "        self.in_features = original_linear.in_features\n",
        "        self.out_features = original_linear.out_features\n",
        "        weight_quantized, weight_scale, weight_zero_point = quantize_tensor_to_fp4(original_linear.weight.data)\n",
        "        self.register_buffer('quantized_weight', weight_quantized.to(torch.int8))\n",
        "        self.register_buffer('weight_scale', weight_scale)\n",
        "        self.register_buffer('weight_zero_point', weight_zero_point)\n",
        "        if original_linear.bias is not None:\n",
        "            bias_quantized, bias_scale, bias_zero_point = quantize_tensor_to_fp4(original_linear.bias.data)\n",
        "            self.register_buffer('quantized_bias', bias_quantized.to(torch.int8))\n",
        "            self.register_buffer('bias_scale', bias_scale)\n",
        "            self.register_buffer('bias_zero_point', bias_zero_point)\n",
        "        else:\n",
        "            self.quantized_bias = None\n",
        "\n",
        "    def forward(self, input_tensor):\n",
        "        weight = (self.quantized_weight.float() - self.weight_zero_point) * self.weight_scale\n",
        "        bias = None if self.quantized_bias is None else (self.quantized_bias.float() - self.bias_zero_point) * self.bias_scale\n",
        "        return F.linear(input_tensor, weight, bias)\n",
        "\n",
        "class FP4Conv2dLayer(nn.Module):\n",
        "    \"\"\"FP4 quantized 2D convolution layer\"\"\"\n",
        "    def __init__(self, original_conv):\n",
        "        super().__init__()\n",
        "        self.in_channels = original_conv.in_channels\n",
        "        self.out_channels = original_conv.out_channels\n",
        "        self.kernel_size = original_conv.kernel_size\n",
        "        self.stride = original_conv.stride\n",
        "        self.padding = original_conv.padding\n",
        "        self.dilation = original_conv.dilation\n",
        "        self.groups = original_conv.groups\n",
        "        weight_quantized, weight_scale, weight_zero_point = quantize_tensor_to_fp4(original_conv.weight.data)\n",
        "        self.register_buffer('quantized_weight', weight_quantized.to(torch.int8))\n",
        "        self.register_buffer('weight_scale', weight_scale)\n",
        "        self.register_buffer('weight_zero_point', weight_zero_point)\n",
        "        if original_conv.bias is not None:\n",
        "            bias_quantized, bias_scale, bias_zero_point = quantize_tensor_to_fp4(original_conv.bias.data)\n",
        "            self.register_buffer('quantized_bias', bias_quantized.to(torch.int8))\n",
        "            self.register_buffer('bias_scale', bias_scale)\n",
        "            self.register_buffer('bias_zero_point', bias_zero_point)\n",
        "        else:\n",
        "            self.quantized_bias = None\n",
        "\n",
        "    def forward(self, input_tensor):\n",
        "        weight = (self.quantized_weight.float() - self.weight_zero_point) * self.weight_scale\n",
        "        bias = None if self.quantized_bias is None else (self.quantized_bias.float() - self.bias_zero_point) * self.bias_scale\n",
        "        return F.conv2d(input_tensor, weight, bias, self.stride, self.padding, self.dilation, self.groups)\n",
        "\n",
        "def quantize_model_to_fp4(model, quantize_backbone=True):\n",
        "    \"\"\"Quantize large Linear and Conv2d layers to FP4, skipping sensitive modules\"\"\"\n",
        "    quantized_layers = []\n",
        "    skip_modules = ['pos_embed', 'cls_token']\n",
        "    for name, module in model.named_children():\n",
        "        skip_this_module = any(skip_name in name.lower() for skip_name in skip_modules)\n",
        "        if not skip_this_module:\n",
        "            if isinstance(module, nn.Linear) and module.weight.numel() > 1000:\n",
        "                setattr(model, name, FP4LinearLayer(module))\n",
        "                quantized_layers.append(f\"Linear: {name} ({module.weight.shape})\")\n",
        "            elif isinstance(module, nn.Conv2d) and module.weight.numel() > 1000:\n",
        "                setattr(model, name, FP4Conv2dLayer(module))\n",
        "                quantized_layers.append(f\"Conv2d: {name} ({module.weight.shape})\")\n",
        "            else:\n",
        "                sub_quantized = quantize_model_to_fp4(module, quantize_backbone)\n",
        "                quantized_layers.extend(sub_quantized)\n",
        "    return quantized_layers"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2oqlSIGLavkx"
      },
      "outputs": [],
      "source": [
        "def fix_pos_embed_shape(model):\n",
        "    \"\"\"Force positional embedding into correct shape before interpolation\"\"\"\n",
        "    if hasattr(model, 'pos_embed'):\n",
        "        pos_embed = model.pos_embed\n",
        "        print(f\"Original pos_embed shape: {pos_embed.shape}\")\n",
        "        try:\n",
        "            if pos_embed.dim() == 3 and pos_embed.shape[1] == 32 and pos_embed.shape[2] == 32:  # [C, H, W]\n",
        "                pos_embed = pos_embed.unsqueeze(0)  # Add batch dimension: [1, C, H, W]\n",
        "            elif pos_embed.dim() == 3:  # [B, N, C]\n",
        "                B, N, C = pos_embed.shape\n",
        "                H = W = int(math.sqrt(N))\n",
        "                pos_embed = pos_embed.transpose(1, 2).reshape(B, C, H, W)\n",
        "            elif pos_embed.dim() == 2:  # [N, C]\n",
        "                N, C = pos_embed.shape\n",
        "                H = W = int(math.sqrt(N))\n",
        "                pos_embed = pos_embed.reshape(1, C, H, W)\n",
        "            elif pos_embed.dim() == 4 and pos_embed.shape[0] != 1:\n",
        "                pos_embed = pos_embed.unsqueeze(0)\n",
        "            model.pos_embed = torch.nn.Parameter(pos_embed)\n",
        "            print(f\"Fixed pos_embed shape: {model.pos_embed.shape}\")\n",
        "        except Exception as e:\n",
        "            print(f\"Error fixing pos_embed shape: {e}\")\n",
        "            return False\n",
        "    return True\n",
        "\n",
        "def adjust_positional_embedding(model):\n",
        "    \"\"\"Adjust positional embedding dimensions with 2D interpolation\"\"\"\n",
        "    if not hasattr(model, 'backbone') or not hasattr(model, 'linear'):\n",
        "        print(\"Skipping positional embedding adjustment: Model lacks backbone or linear attributes\")\n",
        "        return False\n",
        "\n",
        "    if not hasattr(model, 'pos_embed'):\n",
        "        print(\"No pos_embed found in model\")\n",
        "        return False\n",
        "\n",
        "    try:\n",
        "        device = next(model.parameters()).device\n",
        "        test_input = torch.randn(1, 3, 224, 224).to(device)\n",
        "        backbone_features = model.backbone(test_input)\n",
        "        linear_features = model.linear(backbone_features)\n",
        "        expected_height, expected_width = linear_features.shape[2], linear_features.shape[3]\n",
        "\n",
        "        pos_embed = model.pos_embed\n",
        "        print(f\"Current pos_embed shape: {pos_embed.shape}\")\n",
        "\n",
        "        # Ensure pos_embed is in [B, C, H, W] format\n",
        "        if pos_embed.dim() == 4:\n",
        "            current_height, current_width = pos_embed.shape[2], pos_embed.shape[3]\n",
        "        else:\n",
        "            print(f\"Invalid pos_embed dimension: {pos_embed.dim()}\")\n",
        "            return False\n",
        "\n",
        "        if current_height != expected_height or current_width != expected_width:\n",
        "            print(f\"ðŸ”§ Adjusting pos_embed: {pos_embed.shape} -> expected spatial: ({expected_height}, {expected_width})\")\n",
        "            new_pos_embed = F.interpolate(\n",
        "                pos_embed,\n",
        "                size=(expected_height, expected_width),\n",
        "                mode='bilinear',\n",
        "                align_corners=False\n",
        "            )\n",
        "            model.pos_embed = nn.Parameter(new_pos_embed)\n",
        "            print(f\"Adjusted pos_embed: {pos_embed.shape} -> {model.pos_embed.shape}\")\n",
        "            return True\n",
        "        else:\n",
        "            print(\"pos_embed dimensions already correct\")\n",
        "            return True\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"Error adjusting pos_embed: {e}\")\n",
        "        return False\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AJRfaT1FaziR"
      },
      "outputs": [],
      "source": [
        "def perform_gazelle_inference(model, images, bboxes):\n",
        "    \"\"\"Perform inference with handling for missing positional embeddings\"\"\"\n",
        "    model.eval()\n",
        "\n",
        "    try:\n",
        "        device = next(model.parameters()).device\n",
        "    except StopIteration:\n",
        "        device = next(model.buffers()).device\n",
        "\n",
        "    if not isinstance(images, torch.Tensor):\n",
        "        images = torch.tensor(images)\n",
        "    images = images.to(device)\n",
        "\n",
        "    with torch.no_grad():\n",
        "        try:\n",
        "            input_dict = {\"images\": images, \"bboxes\": bboxes}\n",
        "            output = model(input_dict)\n",
        "            return output, \"full_inference\"\n",
        "\n",
        "        except AttributeError as e:\n",
        "            if \"pos_embed\" in str(e):\n",
        "                return perform_fallback_inference(model, images, bboxes)\n",
        "            else:\n",
        "                return perform_fallback_inference(model, images, bboxes)\n",
        "        except Exception:\n",
        "            return perform_fallback_inference(model, images, bboxes)\n",
        "\n",
        "def perform_fallback_inference(model, images, bboxes):\n",
        "    \"\"\"Fallback inference for models with missing components\"\"\"\n",
        "    try:\n",
        "        if hasattr(model, 'backbone') and hasattr(model, 'linear'):\n",
        "            backbone_output = model.backbone(images)\n",
        "            projected_output = model.linear(backbone_output)\n",
        "            batch_size = projected_output.shape[0]\n",
        "\n",
        "            # Generate reasonable outputs\n",
        "            if hasattr(model, 'inout_head'):\n",
        "                inout = torch.sigmoid(model.inout_head(projected_output.mean(dim=[2, 3])))\n",
        "            else:\n",
        "                inout = torch.sigmoid(torch.randn(batch_size, 1).to(projected_output.device))\n",
        "\n",
        "            if hasattr(model, 'heatmap_head'):\n",
        "                heatmap = model.heatmap_head(projected_output)\n",
        "            else:\n",
        "                heatmap = torch.sigmoid(F.adaptive_avg_pool2d(projected_output, (32, 32)))\n",
        "\n",
        "            return {\n",
        "                'heatmap': heatmap,\n",
        "                'inout': inout,\n",
        "                'features': projected_output\n",
        "            }, \"partial_inference\"\n",
        "\n",
        "    except Exception:\n",
        "        return None, \"failed\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ruZoEXuba2G1"
      },
      "outputs": [],
      "source": [
        "def visualize_heatmap_on_image(image, heatmap, alpha=0.6):\n",
        "    \"\"\"Overlay heatmap on input image for visualization\"\"\"\n",
        "    import cv2\n",
        "    if isinstance(image, Image.Image):\n",
        "        image = np.array(image)\n",
        "    elif isinstance(image, torch.Tensor):\n",
        "        image = image.permute(1, 2, 0).cpu().numpy()\n",
        "        image = (image * 255).astype(np.uint8)\n",
        "    if isinstance(heatmap, torch.Tensor):\n",
        "        heatmap = heatmap.cpu().numpy()\n",
        "    heatmap = cv2.resize(heatmap, (image.shape[1], image.shape[0]))\n",
        "    heatmap = (heatmap - heatmap.min()) / (heatmap.max() - heatmap.min() + 1e-8)\n",
        "    heatmap = (heatmap * 255).astype(np.uint8)\n",
        "    heatmap = cv2.applyColorMap(heatmap, cv2.COLORMAP_JET)\n",
        "    overlay = cv2.addWeighted(image, 1 - alpha, heatmap, alpha, 0)\n",
        "    return overlay"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "o6KIsqVba4Rn"
      },
      "outputs": [],
      "source": [
        "def analyze_model_memory_usage(model, name=\"Model\"):\n",
        "    \"\"\"Calculate model memory usage, accounting for FP4 quantization\"\"\"\n",
        "    total_parameters = 0\n",
        "    quantized_parameters = 0\n",
        "    total_size_bytes = 0\n",
        "\n",
        "    # Count parameters\n",
        "    for param_name, param in model.named_parameters():\n",
        "        total_parameters += param.numel()\n",
        "        total_size_bytes += param.numel() * param.element_size()\n",
        "\n",
        "    # Count buffers\n",
        "    for buffer_name, buffer in model.named_buffers():\n",
        "        if 'quantized_weight' in buffer_name or 'quantized_bias' in buffer_name:\n",
        "            quantized_parameters += buffer.numel()\n",
        "            total_size_bytes += buffer.numel() * 0.5  # FP4 stored as int8\n",
        "        else:\n",
        "            total_size_bytes += buffer.numel() * buffer.element_size()\n",
        "\n",
        "    total_size_mb = total_size_bytes / (1024 * 1024)\n",
        "    print(f\"\\n {name} Analysis:\")\n",
        "    print(f\"Total parameters: {total_parameters:,}\")\n",
        "    print(f\"Quantized parameters: {quantized_parameters:,}\")\n",
        "    print(f\"Model size: {total_size_mb:.2f} MB\")\n",
        "    return total_parameters, quantized_parameters, total_size_mb"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "W6s_4tkIa7TP"
      },
      "outputs": [],
      "source": [
        "def benchmark_inference(model, num_runs=100):\n",
        "    \"\"\"Benchmark model inference time\"\"\"\n",
        "    try:\n",
        "        device = next(model.parameters()).device\n",
        "    except StopIteration:\n",
        "        device = next(model.buffers()).device\n",
        "\n",
        "    test_images = torch.randn(1, 3, 224, 224).to(device)\n",
        "    test_bboxes = [[(0.2, 0.2, 0.6, 0.6)]]\n",
        "    model.eval()\n",
        "\n",
        "    # Warmup run\n",
        "    with torch.no_grad():\n",
        "        perform_gazelle_inference(model, test_images, test_bboxes)\n",
        "\n",
        "    # Timed runs\n",
        "    if device.type == 'cuda':\n",
        "        torch.cuda.synchronize()\n",
        "        start_event = torch.cuda.Event(enable_timing=True)\n",
        "        end_event = torch.cuda.Event(enable_timing=True)\n",
        "        start_event.record()\n",
        "\n",
        "        for _ in range(num_runs):\n",
        "            with torch.no_grad():\n",
        "                perform_gazelle_inference(model, test_images, test_bboxes)\n",
        "\n",
        "        end_event.record()\n",
        "        torch.cuda.synchronize()\n",
        "        total_time = start_event.elapsed_time(end_event) / 1000.0\n",
        "    else:\n",
        "        import time\n",
        "        start_time = time.time()\n",
        "        for _ in range(num_runs):\n",
        "            with torch.no_grad():\n",
        "                perform_gazelle_inference(model, test_images, test_bboxes)\n",
        "        total_time = time.time() - start_time\n",
        "\n",
        "    average_time = total_time / num_runs\n",
        "    fps = 1.0 / average_time\n",
        "    print(f\"{model.__class__.__name__} Inference time: {average_time*1000:.2f}ms per frame ({fps:.1f} FPS)\")\n",
        "    return average_time"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "D6smOMnYbBec"
      },
      "outputs": [],
      "source": [
        "def quantize_gazelle_model():\n",
        "    \"\"\"Run FP4 quantization on Gazelle model\"\"\"\n",
        "\n",
        "    print(\"Starting FP4 Quantization Pipeline...\")\n",
        "\n",
        "    # Select device\n",
        "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "    print(f\"Using device: {device}\")\n",
        "\n",
        "    # Load Gazelle model\n",
        "    try:\n",
        "        print(\"Loading Gazelle model via PyTorch Hub...\")\n",
        "        model, transform = torch.hub.load('fkryan/gazelle', 'gazelle_dinov2_vitl14', pretrained=True)\n",
        "        model = model.to(device).eval()\n",
        "        print(\"Model loaded successfully\")\n",
        "    except Exception as e:\n",
        "        print(f\"Failed to load Gazelle model: {e}\")\n",
        "        return None, None, None\n",
        "\n",
        "    # Load pretrained weights\n",
        "    checkpoint_url = \"https://github.com/fkryan/gazelle/releases/download/v1.0.0/gazelle_dinov2_vitl14.pt\"\n",
        "    try:\n",
        "        checkpoint = torch.hub.load_state_dict_from_url(checkpoint_url, map_location=device)\n",
        "        model.load_state_dict(checkpoint, strict=False)\n",
        "        print(\"Loaded pretrained weights from checkpoint\")\n",
        "    except Exception as e:\n",
        "        print(f\"Could not load pretrained weights: {e}\")\n",
        "        print(\"Continuing with PyTorch Hub weights\")\n",
        "\n",
        "    # Adjust positional embeddings\n",
        "    print(\"Adjusting positional embeddings...\")\n",
        "    fix_pos_embed_shape(model)\n",
        "    pos_embed_adjusted = adjust_positional_embedding(model)\n",
        "\n",
        "    # Test original model\n",
        "    print(\"Testing original model...\")\n",
        "    test_images = torch.randn(1, 3, 224, 224).to(device)\n",
        "    test_bboxes = [[(0.2, 0.2, 0.6, 0.6)]]\n",
        "    original_output, original_status = perform_gazelle_inference(model, test_images, test_bboxes)\n",
        "    if original_output is None:\n",
        "        print(\"Original model failed\")\n",
        "    else:\n",
        "        print(f\"Original model works. Status: {original_status}\")\n",
        "        if isinstance(original_output, dict):\n",
        "            for k, v in original_output.items():\n",
        "                if hasattr(v, 'shape'):\n",
        "                    print(f\"  {k}: {v.shape}\")\n",
        "\n",
        "    # Memory usage before quantization\n",
        "    original_params, original_quantized_params, original_size_mb = analyze_model_memory_usage(model, \"Original Model\")\n",
        "\n",
        "    # Apply FP4 quantization\n",
        "    print(\"\\nApplying FP4 Quantization...\")\n",
        "    quantized_model = copy.deepcopy(model)\n",
        "    quantized_layers = quantize_model_to_fp4(quantized_model, quantize_backbone=True)\n",
        "    quantized_model = quantized_model.to(device).eval()\n",
        "\n",
        "    print(f\"Quantized {len(quantized_layers)} layers\")\n",
        "    if len(quantized_layers) > 10:\n",
        "        for layer in quantized_layers[:10]:\n",
        "            print(f\"  {layer}\")\n",
        "        print(f\"  ... and {len(quantized_layers) - 10} more layers\")\n",
        "\n",
        "    # Adjust embeddings for quantized model\n",
        "    fix_pos_embed_shape(quantized_model)\n",
        "    adjust_positional_embedding(quantized_model)\n",
        "\n",
        "    # Memory usage after quantization\n",
        "    quantized_params, quantized_quantized_params, quantized_size_mb = analyze_model_memory_usage(quantized_model, \"FP4 Quantized Model\")\n",
        "\n",
        "    # Save models\n",
        "    torch.save(model.state_dict(), \"/content/gazelle_original_fp32.pth\")\n",
        "    torch.save(quantized_model.state_dict(), \"/content/gazelle_quantized_fp4.pth\")\n",
        "\n",
        "    # Show compression stats\n",
        "    compression_ratio = original_size_mb / quantized_size_mb if quantized_size_mb > 0 else float('inf')\n",
        "    print(\"\\nQuantization Results:\")\n",
        "    print(f\"Size reduction: {((original_size_mb - quantized_size_mb) / original_size_mb) * 100:.1f}%\")\n",
        "    print(f\"Original size: {original_size_mb:.2f} MB\")\n",
        "    print(f\"Quantized size: {quantized_size_mb:.2f} MB\")\n",
        "    print(f\"Compression ratio: {compression_ratio:.2f}x\")\n",
        "    print(f\"Memory saved: {original_size_mb - quantized_size_mb:.1f} MB\")\n",
        "    print(f\"Quantized parameters fraction: {(quantized_quantized_params / max(1, quantized_params + quantized_quantized_params) * 100):.1f}%\")\n",
        "\n",
        "    # Test quantized model\n",
        "    print(\"\\nTesting quantized model...\")\n",
        "    quantized_output, quantized_status = perform_gazelle_inference(quantized_model, test_images, test_bboxes)\n",
        "    if quantized_output is None:\n",
        "        print(\"Quantized model failed\")\n",
        "    else:\n",
        "        print(f\"Quantized model works. Status: {quantized_status}\")\n",
        "        if isinstance(quantized_output, dict):\n",
        "            for k, v in quantized_output.items():\n",
        "                if hasattr(v, 'shape'):\n",
        "                    print(f\"  {k}: {v.shape}\")\n",
        "        if original_output and isinstance(original_output, dict) and isinstance(quantized_output, dict):\n",
        "            print(\"\\nComparing outputs...\")\n",
        "            for k in set(original_output) & set(quantized_output):\n",
        "                if hasattr(original_output[k], 'shape') and hasattr(quantized_output[k], 'shape'):\n",
        "                    if original_output[k].shape == quantized_output[k].shape:\n",
        "                        mse = torch.mean((original_output[k] - quantized_output[k]) ** 2).item()\n",
        "                        print(f\"  MSE for {k}: {mse:.8f}\")\n",
        "\n",
        "    # Return results\n",
        "    saved_data = {\n",
        "        'model_state_dict': quantized_model.state_dict(),\n",
        "        'original_model_state_dict': model.state_dict(),\n",
        "        'model_config': {\n",
        "            'model_type': 'gazelle_dinov2_vitl14_inout',\n",
        "            'quantization_method': 'FP4_aggressive_symmetric',\n",
        "            'original_size_mb': original_size_mb,\n",
        "            'quantized_size_mb': quantized_size_mb,\n",
        "            'compression_ratio': compression_ratio,\n",
        "            'parameters_original': original_params,\n",
        "            'parameters_quantized': quantized_params,\n",
        "            'quantized_parameters_count': quantized_quantized_params,\n",
        "            'pos_embed_adjusted': pos_embed_adjusted,\n",
        "            'quantized_layers_count': len(quantized_layers)\n",
        "        },\n",
        "        'input_format': {\n",
        "            'images_shape': [1, 3, 224, 224],\n",
        "            'bboxes_format': [[(0.2, 0.2, 0.6, 0.6)]],\n",
        "            'description': 'images: tensor [B,3,H,W], bboxes: list of lists with normalized [x1,y1,x2,y2]'\n",
        "        }\n",
        "    }\n",
        "\n",
        "    return model, quantized_model, saved_data\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tluGAFvIbDtb",
        "outputId": "e5d10680-2e6f-4721-993e-f5b4db759f1d"
      },
      "outputs": [],
      "source": [
        "if __name__ == \"__main__\":\n",
        "    try:\n",
        "        original_model, quantized_model, saved_data = quantize_gazelle_model()\n",
        "    except Exception as e:\n",
        "        print(f\"Pipeline failed: {e}\")\n",
        "        import traceback\n",
        "        traceback.print_exc()"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
