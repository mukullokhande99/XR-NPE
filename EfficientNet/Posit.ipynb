{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "39504586-25bd-4800-85eb-6263bf1375bb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Posit support found in QPyTorch\n",
      "Using device: cuda\n"
     ]
    }
   ],
   "source": [
    "# Core imports\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader\n",
    "import torchvision.transforms as transforms\n",
    "import torchvision.datasets as datasets\n",
    "from efficientnet_pytorch import EfficientNet\n",
    "from tqdm import tqdm\n",
    "\n",
    "# QPyTorch imports\n",
    "from qtorch.quant import Quantizer, quantizer\n",
    "from qtorch import FixedPoint, BlockFloatingPoint, FloatingPoint\n",
    "from qtorch.optim import OptimLP\n",
    "from qtorch.auto_low import lower\n",
    "\n",
    "# Try importing Posit from forked QPyTorch\n",
    "try:\n",
    "    from qtorch import Posit\n",
    "    HAS_POSIT = True\n",
    "    print(\" Posit support found in QPyTorch\")\n",
    "except ImportError:\n",
    "    HAS_POSIT = False\n",
    "    print(\" No Posit type found — falling back to FP8-like format\")\n",
    "\n",
    "# Config\n",
    "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "MODEL_NAME = \"efficientnet-b0\"\n",
    "BATCH_SIZE = 64\n",
    "LR = 0.05\n",
    "MOMENTUM = 0.9\n",
    "WEIGHT_DECAY = 5e-4\n",
    "EPOCHS = 10\n",
    "DATA_PATH = \"./data\"\n",
    "\n",
    "torch.manual_seed(42)\n",
    "if DEVICE.type == \"cuda\":\n",
    "    torch.cuda.manual_seed_all(42)\n",
    "\n",
    "print(f\"Using device: {DEVICE}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "b6f03cdc-cde6-4e4f-b512-47977729557c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n",
      "Train samples: 50000, Val samples: 10000\n"
     ]
    }
   ],
   "source": [
    "# ImageNet normalization\n",
    "normalize = transforms.Normalize(mean=[0.485, 0.456, 0.406],\n",
    "                                 std=[0.229, 0.224, 0.225])\n",
    "\n",
    "train_tf = transforms.Compose([\n",
    "    transforms.Resize(256),\n",
    "    transforms.RandomResizedCrop(224),\n",
    "    transforms.RandomHorizontalFlip(),\n",
    "    transforms.ToTensor(),\n",
    "    normalize,\n",
    "])\n",
    "val_tf = transforms.Compose([\n",
    "    transforms.Resize(256),\n",
    "    transforms.CenterCrop(224),\n",
    "    transforms.ToTensor(),\n",
    "    normalize,\n",
    "])\n",
    "\n",
    "# CIFAR100 for demo\n",
    "train_set = datasets.CIFAR10(root=DATA_PATH, train=True, download=True, transform=train_tf)\n",
    "val_set   = datasets.CIFAR10(root=DATA_PATH, train=False, download=True, transform=val_tf)\n",
    "\n",
    "train_loader = DataLoader(train_set, batch_size=BATCH_SIZE, shuffle=True,\n",
    "                          num_workers=4, pin_memory=True, persistent_workers=True)\n",
    "val_loader = DataLoader(val_set, batch_size=BATCH_SIZE, shuffle=False,\n",
    "                        num_workers=4, pin_memory=True, persistent_workers=True)\n",
    "\n",
    "NUM_CLASSES = 10\n",
    "print(f\"Train samples: {len(train_set)}, Val samples: {len(val_set)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "ed5256c2-24fe-4573-8114-1591c49d04cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define number formats\n",
    "def make_quant_numbers():\n",
    "    if HAS_POSIT:\n",
    "        W_NUM   = Posit(8, 1)\n",
    "        A_NUM   = Posit(8, 1)\n",
    "        WG_NUM  = Posit(8, 1)\n",
    "        AG_NUM  = Posit(8, 1)\n",
    "    else:\n",
    "        W_NUM   = FloatingPoint(exp=3, man=4)\n",
    "        A_NUM   = FloatingPoint(exp=3, man=4)\n",
    "        WG_NUM  = FloatingPoint(exp=5, man=10)\n",
    "        AG_NUM  = FloatingPoint(exp=5, man=10)\n",
    "    return W_NUM, A_NUM, WG_NUM, AG_NUM\n",
    "\n",
    "# Apply quantization to conv and linear layers\n",
    "def quantize_model(net):\n",
    "    # Example: both weights and activations quantized to 8 bits\n",
    "    W_NUM, A_NUM, WG_NUM, AG_NUM = make_quant_numbers()\n",
    "\n",
    "    qnet = lower(\n",
    "        net,\n",
    "        layer_types=[\"conv\", \"linear\"],   # layers to quantize\n",
    "        forward_number=W_NUM,             # number of bits for forward pass (weights + activations)\n",
    "        backward_number=WG_NUM,           # number of bits for backward pass (gradients)\n",
    "        forward_rounding=\"nearest\",       # rounding method for forward pass\n",
    "        backward_rounding=\"nearest\"       # rounding method for backward pass\n",
    "    )\n",
    "    return qnet\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "787edb9e-f10c-44b4-a7cf-186462698b89",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(model, layer_types=[], forward_number=None, backward_number=None, forward_rounding='stochastic', backward_rounding='stochastic')\n"
     ]
    }
   ],
   "source": [
    "import inspect\n",
    "from qtorch.quant import quantizer  # replace with actual import path\n",
    "print(inspect.signature(lower))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "01ef073a-36e6-4925-9bc1-aafd0674401a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded pretrained weights for efficientnet-b0\n",
      "Loaded pretrained efficientnet-b0\n",
      "Parameters before classifier change: 5,288,548\n",
      "Modified classifier: 1280 -> 10 classes\n",
      "Parameters after classifier change: 4,020,358\n",
      "Parameters after assignment to self.net: 4,020,358\n",
      "Model parameters after creation: 4,020,358\n",
      "Model parameters after moving to device: 4,020,358\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from efficientnet_pytorch import EfficientNet\n",
    "from qtorch import FloatingPoint\n",
    "from qtorch.auto_low import lower\n",
    "\n",
    "class QuantizedEfficientNet(nn.Module):\n",
    "    def __init__(self, model_name, num_classes, pretrained=True):\n",
    "        super().__init__()\n",
    "        \n",
    "        # Load base EfficientNet model\n",
    "        if pretrained:\n",
    "            base = EfficientNet.from_pretrained(model_name)\n",
    "            print(f\"Loaded pretrained {model_name}\")\n",
    "        else:\n",
    "            base = EfficientNet.from_name(model_name)\n",
    "            print(f\"Created {model_name} from scratch\")\n",
    "        \n",
    "        # Check parameters before modification\n",
    "        print(f\"Parameters before classifier change: {sum(p.numel() for p in base.parameters()):,}\")\n",
    "        \n",
    "        # Modify classifier for target number of classes\n",
    "        in_features = base._fc.in_features\n",
    "        base._fc = nn.Linear(in_features, num_classes)\n",
    "        print(f\"Modified classifier: {in_features} -> {num_classes} classes\")\n",
    "        print(f\"Parameters after classifier change: {sum(p.numel() for p in base.parameters()):,}\")\n",
    "        \n",
    "        # Store the base model WITHOUT quantization first\n",
    "        self.net = base\n",
    "        print(f\"Parameters after assignment to self.net: {sum(p.numel() for p in self.net.parameters()):,}\")\n",
    "    \n",
    "    def apply_quantization_later(self):\n",
    "        \"\"\"Apply quantization after model is fully created\"\"\"\n",
    "        print(\"Applying quantization...\")\n",
    "        print(f\"Parameters before quantization: {sum(p.numel() for p in self.net.parameters()):,}\")\n",
    "        \n",
    "        try:\n",
    "            self.net = lower(\n",
    "                self.net,\n",
    "                layer_types=['conv', 'linear'],\n",
    "                forward_number=FloatingPoint(exp=3, man=4),\n",
    "                backward_number=FloatingPoint(exp=5, man=10),\n",
    "                forward_rounding=\"nearest\",\n",
    "                backward_rounding=\"nearest\"\n",
    "            )\n",
    "            print(f\"Parameters after quantization: {sum(p.numel() for p in self.net.parameters()):,}\")\n",
    "        except Exception as e:\n",
    "            print(f\"Quantization failed: {e}\")\n",
    "    \n",
    "    def forward(self, x):\n",
    "        return self.net(x)\n",
    "\n",
    "# Create the model WITHOUT quantization first\n",
    "MODEL_NAME = 'efficientnet-b0'\n",
    "NUM_CLASSES = 10\n",
    "DEVICE = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "model = QuantizedEfficientNet(MODEL_NAME, NUM_CLASSES, pretrained=True)\n",
    "print(f\"Model parameters after creation: {sum(p.numel() for p in model.parameters()):,}\")\n",
    "\n",
    "# Move to device\n",
    "model = model.to(DEVICE)\n",
    "print(f\"Model parameters after moving to device: {sum(p.numel() for p in model.parameters()):,}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "fbf0fdb3-cf98-4249-968b-f1bf1844f875",
   "metadata": {},
   "outputs": [],
   "source": [
    "@torch.no_grad()\n",
    "def validate(model, loader, device, criterion):\n",
    "    model.eval()\n",
    "    total_loss, correct, total = 0.0, 0, 0\n",
    "    for inputs, targets in tqdm(loader, desc=\"Val\", leave=False):\n",
    "        inputs, targets = inputs.to(device), targets.to(device)\n",
    "        outputs = model(inputs)\n",
    "        loss = criterion(outputs, targets)\n",
    "        total_loss += loss.item() * targets.size(0)\n",
    "        correct += (outputs.argmax(1) == targets).sum().item()\n",
    "        total += targets.size(0)\n",
    "    return total_loss / total, 100.0 * correct / total\n",
    "\n",
    "def train_epoch(model, loader, device, criterion, optimizer):\n",
    "    model.train()\n",
    "    total_loss, correct, total = 0.0, 0, 0\n",
    "    for inputs, targets in tqdm(loader, desc=\"Train\", leave=False):\n",
    "        inputs, targets = inputs.to(device), targets.to(device)\n",
    "        optimizer.zero_grad(set_to_none=True)\n",
    "        outputs = model(inputs)\n",
    "        loss = criterion(outputs, targets)\n",
    "        loss.backward()\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), 5.0)\n",
    "        optimizer.step()\n",
    "        total_loss += loss.item() * targets.size(0)\n",
    "        correct += (outputs.argmax(1) == targets).sum().item()\n",
    "        total += targets.size(0)\n",
    "    return total_loss / total, 100.0 * correct / total\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7834f708-9415-4001-8d1b-763aaebc44a6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 1/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train:   8%|█████▉                                                                     | 62/782 [01:45<19:07,  1.59s/it]"
     ]
    }
   ],
   "source": [
    "criterion = nn.CrossEntropyLoss()\n",
    "_, _, WG_NUM, _ = make_quant_numbers()\n",
    "\n",
    "# Create base optimizer\n",
    "base_optimizer = optim.SGD(\n",
    "    model.parameters(),\n",
    "    lr=LR,\n",
    "    momentum=MOMENTUM,\n",
    "    weight_decay=WEIGHT_DECAY\n",
    ")\n",
    "\n",
    "# Create quantizers for different components\n",
    "weight_quantizer = quantizer(forward_number=WG_NUM, forward_rounding=\"nearest\")\n",
    "grad_quantizer = quantizer(forward_number=WG_NUM, forward_rounding=\"nearest\")\n",
    "momentum_quantizer = quantizer(forward_number=WG_NUM, forward_rounding=\"nearest\")\n",
    "\n",
    "# Wrap with OptimLP with full quantization\n",
    "optimizer = OptimLP(\n",
    "    optim=base_optimizer,\n",
    "    weight_quant=weight_quantizer,  # Quantize weights\n",
    "    grad_quant=grad_quantizer,      # Quantize gradients\n",
    "    momentum_quant=momentum_quantizer,  # Quantize momentum\n",
    "    grad_scaling=1.0,\n",
    "    acc_quant=None  # Can add accumulator quantization if needed\n",
    ")\n",
    "\n",
    "scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=EPOCHS)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "best_acc = 0.0\n",
    "for epoch in range(1, EPOCHS + 1):\n",
    "    print(f\"\\nEpoch {epoch}/{EPOCHS}\")\n",
    "    train_loss, train_acc = train_epoch(model, train_loader, DEVICE, criterion, optimizer)\n",
    "    val_loss, val_acc = validate(model, val_loader, DEVICE, criterion)\n",
    "    scheduler.step()\n",
    "\n",
    "    print(f\"Train  | Loss: {train_loss:.4f} | Acc: {train_acc:.2f}%\")\n",
    "    print(f\"Val    | Loss: {val_loss:.4f} | Acc: {val_acc:.2f}%\")\n",
    "    print(f\"LR     | {optimizer.param_groups[0]['lr']:.6f}\")\n",
    "\n",
    "    if val_acc > best_acc:\n",
    "        best_acc = val_acc\n",
    "        torch.save(model.state_dict(), \"best_posit8_efficientnet.pth\")\n",
    "        print(f\"✅ New best model saved with Acc: {best_acc:.2f}%\")\n",
    "\n",
    "print(f\"\\nTraining completed. Best Val Acc: {best_acc:.2f}%\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6ff5a0c-ffc8-44d7-8232-acff46ec0a34",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.23"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
