{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a2a497a4-203a-4e88-86e0-835d29736ac1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting efficientnet-pytorch\n",
      "  Downloading efficientnet_pytorch-0.7.1.tar.gz (21 kB)\n",
      "  Preparing metadata (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25hRequirement already satisfied: torch in /home/tejas/miniconda3/envs/pilotnet-torch/lib/python3.9/site-packages (from efficientnet-pytorch) (2.2.2+cu118)\n",
      "Requirement already satisfied: filelock in /home/tejas/miniconda3/envs/pilotnet-torch/lib/python3.9/site-packages (from torch->efficientnet-pytorch) (3.13.1)\n",
      "Requirement already satisfied: typing-extensions>=4.8.0 in /home/tejas/miniconda3/envs/pilotnet-torch/lib/python3.9/site-packages (from torch->efficientnet-pytorch) (4.12.2)\n",
      "Requirement already satisfied: sympy in /home/tejas/miniconda3/envs/pilotnet-torch/lib/python3.9/site-packages (from torch->efficientnet-pytorch) (1.13.1)\n",
      "Requirement already satisfied: networkx in /home/tejas/miniconda3/envs/pilotnet-torch/lib/python3.9/site-packages (from torch->efficientnet-pytorch) (3.2.1)\n",
      "Requirement already satisfied: jinja2 in /home/tejas/miniconda3/envs/pilotnet-torch/lib/python3.9/site-packages (from torch->efficientnet-pytorch) (3.1.4)\n",
      "Requirement already satisfied: fsspec in /home/tejas/miniconda3/envs/pilotnet-torch/lib/python3.9/site-packages (from torch->efficientnet-pytorch) (2024.6.1)\n",
      "Requirement already satisfied: nvidia-cuda-nvrtc-cu11==11.8.89 in /home/tejas/miniconda3/envs/pilotnet-torch/lib/python3.9/site-packages (from torch->efficientnet-pytorch) (11.8.89)\n",
      "Requirement already satisfied: nvidia-cuda-runtime-cu11==11.8.89 in /home/tejas/miniconda3/envs/pilotnet-torch/lib/python3.9/site-packages (from torch->efficientnet-pytorch) (11.8.89)\n",
      "Requirement already satisfied: nvidia-cuda-cupti-cu11==11.8.87 in /home/tejas/miniconda3/envs/pilotnet-torch/lib/python3.9/site-packages (from torch->efficientnet-pytorch) (11.8.87)\n",
      "Requirement already satisfied: nvidia-cudnn-cu11==8.7.0.84 in /home/tejas/miniconda3/envs/pilotnet-torch/lib/python3.9/site-packages (from torch->efficientnet-pytorch) (8.7.0.84)\n",
      "Requirement already satisfied: nvidia-cublas-cu11==11.11.3.6 in /home/tejas/miniconda3/envs/pilotnet-torch/lib/python3.9/site-packages (from torch->efficientnet-pytorch) (11.11.3.6)\n",
      "Requirement already satisfied: nvidia-cufft-cu11==10.9.0.58 in /home/tejas/miniconda3/envs/pilotnet-torch/lib/python3.9/site-packages (from torch->efficientnet-pytorch) (10.9.0.58)\n",
      "Requirement already satisfied: nvidia-curand-cu11==10.3.0.86 in /home/tejas/miniconda3/envs/pilotnet-torch/lib/python3.9/site-packages (from torch->efficientnet-pytorch) (10.3.0.86)\n",
      "Requirement already satisfied: nvidia-cusolver-cu11==11.4.1.48 in /home/tejas/miniconda3/envs/pilotnet-torch/lib/python3.9/site-packages (from torch->efficientnet-pytorch) (11.4.1.48)\n",
      "Requirement already satisfied: nvidia-cusparse-cu11==11.7.5.86 in /home/tejas/miniconda3/envs/pilotnet-torch/lib/python3.9/site-packages (from torch->efficientnet-pytorch) (11.7.5.86)\n",
      "Requirement already satisfied: nvidia-nccl-cu11==2.19.3 in /home/tejas/miniconda3/envs/pilotnet-torch/lib/python3.9/site-packages (from torch->efficientnet-pytorch) (2.19.3)\n",
      "Requirement already satisfied: nvidia-nvtx-cu11==11.8.86 in /home/tejas/miniconda3/envs/pilotnet-torch/lib/python3.9/site-packages (from torch->efficientnet-pytorch) (11.8.86)\n",
      "Requirement already satisfied: triton==2.2.0 in /home/tejas/miniconda3/envs/pilotnet-torch/lib/python3.9/site-packages (from torch->efficientnet-pytorch) (2.2.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /home/tejas/miniconda3/envs/pilotnet-torch/lib/python3.9/site-packages (from jinja2->torch->efficientnet-pytorch) (2.1.5)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /home/tejas/miniconda3/envs/pilotnet-torch/lib/python3.9/site-packages (from sympy->torch->efficientnet-pytorch) (1.3.0)\n",
      "Building wheels for collected packages: efficientnet-pytorch\n",
      "\u001b[33m  DEPRECATION: Building 'efficientnet-pytorch' using the legacy setup.py bdist_wheel mechanism, which will be removed in a future version. pip 25.3 will enforce this behaviour change. A possible replacement is to use the standardized build interface by setting the `--use-pep517` option, (possibly combined with `--no-build-isolation`), or adding a `pyproject.toml` file to the source tree of 'efficientnet-pytorch'. Discussion can be found at https://github.com/pypa/pip/issues/6334\u001b[0m\u001b[33m\n",
      "\u001b[0m  Building wheel for efficientnet-pytorch (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for efficientnet-pytorch: filename=efficientnet_pytorch-0.7.1-py3-none-any.whl size=16477 sha256=765ca61a4f74d71bbcac0330eb6caf38b0a61094bc8a9b5e7907e8c6280eb061\n",
      "  Stored in directory: /home/tejas/.cache/pip/wheels/29/16/24/752e89d88d333af39a288421e64d613b5f652918e39ef1f8e3\n",
      "Successfully built efficientnet-pytorch\n",
      "Installing collected packages: efficientnet-pytorch\n",
      "Successfully installed efficientnet-pytorch-0.7.1\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m25.1.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.2\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n",
      "Collecting pytorch-quantization\n",
      "  Downloading pytorch-quantization-2.2.1.tar.gz (6.8 kB)\n",
      "  Preparing metadata (setup.py) ... \u001b[?25lerror\n",
      "  \u001b[1;31merror\u001b[0m: \u001b[1msubprocess-exited-with-error\u001b[0m\n",
      "  \n",
      "  \u001b[31m×\u001b[0m \u001b[32mpython setup.py egg_info\u001b[0m did not run successfully.\n",
      "  \u001b[31m│\u001b[0m exit code: \u001b[1;36m1\u001b[0m\n",
      "  \u001b[31m╰─>\u001b[0m \u001b[31m[16 lines of output]\u001b[0m\n",
      "  \u001b[31m   \u001b[0m Traceback (most recent call last):\n",
      "  \u001b[31m   \u001b[0m   File \"<string>\", line 2, in <module>\n",
      "  \u001b[31m   \u001b[0m   File \"<pip-setuptools-caller>\", line 35, in <module>\n",
      "  \u001b[31m   \u001b[0m   File \"/tmp/pip-install-vbbwe381/pytorch-quantization_829e3f1c25144c77ae3a7fd54a4e1c81/setup.py\", line 137, in <module>\n",
      "  \u001b[31m   \u001b[0m     raise RuntimeError(open(\"ERROR.txt\", \"r\").read())\n",
      "  \u001b[31m   \u001b[0m RuntimeError:\n",
      "  \u001b[31m   \u001b[0m ###########################################################################################\n",
      "  \u001b[31m   \u001b[0m The package you are trying to install is only a placeholder project on PyPI.org repository.\n",
      "  \u001b[31m   \u001b[0m This package is hosted on NVIDIA Python Package Index.\n",
      "  \u001b[31m   \u001b[0m \n",
      "  \u001b[31m   \u001b[0m This package can be installed as:\n",
      "  \u001b[31m   \u001b[0m ```\n",
      "  \u001b[31m   \u001b[0m $ pip install --no-cache-dir --extra-index-url https://pypi.nvidia.com pytorch-quantization\n",
      "  \u001b[31m   \u001b[0m ```\n",
      "  \u001b[31m   \u001b[0m ###########################################################################################\n",
      "  \u001b[31m   \u001b[0m \n",
      "  \u001b[31m   \u001b[0m \u001b[31m[end of output]\u001b[0m\n",
      "  \n",
      "  \u001b[1;35mnote\u001b[0m: This error originates from a subprocess, and is likely not a problem with pip.\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m25.1.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.2\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n",
      "\u001b[1;31merror\u001b[0m: \u001b[1mmetadata-generation-failed\u001b[0m\n",
      "\n",
      "\u001b[31m×\u001b[0m Encountered error while generating package metadata.\n",
      "\u001b[31m╰─>\u001b[0m See above for output.\n",
      "\n",
      "\u001b[1;35mnote\u001b[0m: This is an issue with the package mentioned above, not pip.\n",
      "\u001b[1;36mhint\u001b[0m: See above for details.\n",
      "\u001b[?25hCollecting tensorrt\n",
      "  Downloading tensorrt-10.13.2.6.tar.gz (40 kB)\n",
      "  Preparing metadata (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25hCollecting tensorrt_cu13==10.13.2.6 (from tensorrt)\n",
      "  Downloading tensorrt_cu13-10.13.2.6.tar.gz (18 kB)\n",
      "  Preparing metadata (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25hCollecting tensorrt_cu13_libs==10.13.2.6 (from tensorrt_cu13==10.13.2.6->tensorrt)\n",
      "  Downloading tensorrt_cu13_libs-10.13.2.6.tar.gz (704 bytes)\n",
      "  Installing build dependencies ... \u001b[?25ldone\n",
      "\u001b[?25h  Getting requirements to build wheel ... \u001b[?25ldone\n",
      "\u001b[?25h  Preparing metadata (pyproject.toml) ... \u001b[?25ldone\n",
      "\u001b[?25hCollecting tensorrt_cu13_bindings==10.13.2.6 (from tensorrt_cu13==10.13.2.6->tensorrt)\n",
      "  Downloading tensorrt_cu13_bindings-10.13.2.6-cp39-none-manylinux_2_28_x86_64.whl.metadata (606 bytes)\n",
      "Collecting nvidia-cuda-runtime-cu13 (from tensorrt_cu13_libs==10.13.2.6->tensorrt_cu13==10.13.2.6->tensorrt)\n",
      "  Downloading nvidia_cuda_runtime_cu13-0.0.0a0-py2.py3-none-any.whl.metadata (225 bytes)\n",
      "Downloading tensorrt_cu13_bindings-10.13.2.6-cp39-none-manylinux_2_28_x86_64.whl (1.2 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.2/1.2 MB\u001b[0m \u001b[31m8.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_cuda_runtime_cu13-0.0.0a0-py2.py3-none-any.whl (1.2 kB)\n",
      "Building wheels for collected packages: tensorrt, tensorrt_cu13, tensorrt_cu13_libs\n",
      "\u001b[33m  DEPRECATION: Building 'tensorrt' using the legacy setup.py bdist_wheel mechanism, which will be removed in a future version. pip 25.3 will enforce this behaviour change. A possible replacement is to use the standardized build interface by setting the `--use-pep517` option, (possibly combined with `--no-build-isolation`), or adding a `pyproject.toml` file to the source tree of 'tensorrt'. Discussion can be found at https://github.com/pypa/pip/issues/6334\u001b[0m\u001b[33m\n",
      "\u001b[0m  Building wheel for tensorrt (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for tensorrt: filename=tensorrt-10.13.2.6-py2.py3-none-any.whl size=46526 sha256=f4a3660ba6a2ba612acd0609e53e524880bf36bf3bcb2b28602110eadb646767\n",
      "  Stored in directory: /home/tejas/.cache/pip/wheels/7c/43/af/1bd62b72460692f0b02f3159274e016895b7e96d5525e80f1a\n",
      "\u001b[33m  DEPRECATION: Building 'tensorrt_cu13' using the legacy setup.py bdist_wheel mechanism, which will be removed in a future version. pip 25.3 will enforce this behaviour change. A possible replacement is to use the standardized build interface by setting the `--use-pep517` option, (possibly combined with `--no-build-isolation`), or adding a `pyproject.toml` file to the source tree of 'tensorrt_cu13'. Discussion can be found at https://github.com/pypa/pip/issues/6334\u001b[0m\u001b[33m\n",
      "\u001b[0m  Building wheel for tensorrt_cu13 (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for tensorrt_cu13: filename=tensorrt_cu13-10.13.2.6-py2.py3-none-any.whl size=17541 sha256=90c0cd6c1910f57f7cbb391e984f7097537a2d7f29566e65afc4e34df3a10726\n",
      "  Stored in directory: /home/tejas/.cache/pip/wheels/30/10/db/741acd7250b543e12dbbfac5cf313b599735bc4e1eebe45fb5\n",
      "  Building wheel for tensorrt_cu13_libs (pyproject.toml) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for tensorrt_cu13_libs: filename=tensorrt_cu13_libs-10.13.2.6-py2.py3-none-manylinux_2_28_x86_64.whl size=2740427176 sha256=044468b0705728c0c8f0e95325ba881ff3e23bd2eebce0b1e628953ff90bbb5f\n",
      "  Stored in directory: /home/tejas/.cache/pip/wheels/4e/56/7d/97835f3b04011f142baf99154b615d18d4f5f1fd06dbb9e9fe\n",
      "Successfully built tensorrt tensorrt_cu13 tensorrt_cu13_libs\n",
      "Installing collected packages: tensorrt_cu13_bindings, nvidia-cuda-runtime-cu13, tensorrt_cu13_libs, tensorrt_cu13, tensorrt\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5/5\u001b[0m [tensorrt]2/5\u001b[0m [tensorrt_cu13_libs]\n",
      "\u001b[1A\u001b[2KSuccessfully installed nvidia-cuda-runtime-cu13-0.0.0a0 tensorrt-10.13.2.6 tensorrt_cu13-10.13.2.6 tensorrt_cu13_bindings-10.13.2.6 tensorrt_cu13_libs-10.13.2.6\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m25.1.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.2\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n",
      "Collecting onnx\n",
      "  Downloading onnx-1.18.0-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.9 kB)\n",
      "Requirement already satisfied: numpy>=1.22 in /home/tejas/miniconda3/envs/pilotnet-torch/lib/python3.9/site-packages (from onnx) (1.26.4)\n",
      "Requirement already satisfied: protobuf>=4.25.1 in /home/tejas/miniconda3/envs/pilotnet-torch/lib/python3.9/site-packages (from onnx) (6.31.1)\n",
      "Requirement already satisfied: typing_extensions>=4.7.1 in /home/tejas/miniconda3/envs/pilotnet-torch/lib/python3.9/site-packages (from onnx) (4.12.2)\n",
      "Downloading onnx-1.18.0-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (17.6 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m17.6/17.6 MB\u001b[0m \u001b[31m17.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: onnx\n",
      "Successfully installed onnx-1.18.0\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m25.1.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.2\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n",
      "Requirement already satisfied: timm in /home/tejas/miniconda3/envs/pilotnet-torch/lib/python3.9/site-packages (1.0.19)\n",
      "Requirement already satisfied: torch in /home/tejas/miniconda3/envs/pilotnet-torch/lib/python3.9/site-packages (from timm) (2.2.2+cu118)\n",
      "Requirement already satisfied: torchvision in /home/tejas/miniconda3/envs/pilotnet-torch/lib/python3.9/site-packages (from timm) (0.17.2+cu118)\n",
      "Requirement already satisfied: pyyaml in /home/tejas/miniconda3/envs/pilotnet-torch/lib/python3.9/site-packages (from timm) (6.0.2)\n",
      "Requirement already satisfied: huggingface_hub in /home/tejas/miniconda3/envs/pilotnet-torch/lib/python3.9/site-packages (from timm) (0.34.2)\n",
      "Requirement already satisfied: safetensors in /home/tejas/miniconda3/envs/pilotnet-torch/lib/python3.9/site-packages (from timm) (0.5.3)\n",
      "Requirement already satisfied: filelock in /home/tejas/miniconda3/envs/pilotnet-torch/lib/python3.9/site-packages (from huggingface_hub->timm) (3.13.1)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in /home/tejas/miniconda3/envs/pilotnet-torch/lib/python3.9/site-packages (from huggingface_hub->timm) (2024.6.1)\n",
      "Requirement already satisfied: packaging>=20.9 in /home/tejas/miniconda3/envs/pilotnet-torch/lib/python3.9/site-packages (from huggingface_hub->timm) (25.0)\n",
      "Requirement already satisfied: requests in /home/tejas/miniconda3/envs/pilotnet-torch/lib/python3.9/site-packages (from huggingface_hub->timm) (2.32.4)\n",
      "Requirement already satisfied: tqdm>=4.42.1 in /home/tejas/miniconda3/envs/pilotnet-torch/lib/python3.9/site-packages (from huggingface_hub->timm) (4.67.1)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /home/tejas/miniconda3/envs/pilotnet-torch/lib/python3.9/site-packages (from huggingface_hub->timm) (4.12.2)\n",
      "Requirement already satisfied: hf-xet<2.0.0,>=1.1.3 in /home/tejas/miniconda3/envs/pilotnet-torch/lib/python3.9/site-packages (from huggingface_hub->timm) (1.1.5)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in /home/tejas/miniconda3/envs/pilotnet-torch/lib/python3.9/site-packages (from requests->huggingface_hub->timm) (2.1.1)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /home/tejas/miniconda3/envs/pilotnet-torch/lib/python3.9/site-packages (from requests->huggingface_hub->timm) (3.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /home/tejas/miniconda3/envs/pilotnet-torch/lib/python3.9/site-packages (from requests->huggingface_hub->timm) (1.26.13)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /home/tejas/miniconda3/envs/pilotnet-torch/lib/python3.9/site-packages (from requests->huggingface_hub->timm) (2022.12.7)\n",
      "Requirement already satisfied: sympy in /home/tejas/miniconda3/envs/pilotnet-torch/lib/python3.9/site-packages (from torch->timm) (1.13.1)\n",
      "Requirement already satisfied: networkx in /home/tejas/miniconda3/envs/pilotnet-torch/lib/python3.9/site-packages (from torch->timm) (3.2.1)\n",
      "Requirement already satisfied: jinja2 in /home/tejas/miniconda3/envs/pilotnet-torch/lib/python3.9/site-packages (from torch->timm) (3.1.4)\n",
      "Requirement already satisfied: nvidia-cuda-nvrtc-cu11==11.8.89 in /home/tejas/miniconda3/envs/pilotnet-torch/lib/python3.9/site-packages (from torch->timm) (11.8.89)\n",
      "Requirement already satisfied: nvidia-cuda-runtime-cu11==11.8.89 in /home/tejas/miniconda3/envs/pilotnet-torch/lib/python3.9/site-packages (from torch->timm) (11.8.89)\n",
      "Requirement already satisfied: nvidia-cuda-cupti-cu11==11.8.87 in /home/tejas/miniconda3/envs/pilotnet-torch/lib/python3.9/site-packages (from torch->timm) (11.8.87)\n",
      "Requirement already satisfied: nvidia-cudnn-cu11==8.7.0.84 in /home/tejas/miniconda3/envs/pilotnet-torch/lib/python3.9/site-packages (from torch->timm) (8.7.0.84)\n",
      "Requirement already satisfied: nvidia-cublas-cu11==11.11.3.6 in /home/tejas/miniconda3/envs/pilotnet-torch/lib/python3.9/site-packages (from torch->timm) (11.11.3.6)\n",
      "Requirement already satisfied: nvidia-cufft-cu11==10.9.0.58 in /home/tejas/miniconda3/envs/pilotnet-torch/lib/python3.9/site-packages (from torch->timm) (10.9.0.58)\n",
      "Requirement already satisfied: nvidia-curand-cu11==10.3.0.86 in /home/tejas/miniconda3/envs/pilotnet-torch/lib/python3.9/site-packages (from torch->timm) (10.3.0.86)\n",
      "Requirement already satisfied: nvidia-cusolver-cu11==11.4.1.48 in /home/tejas/miniconda3/envs/pilotnet-torch/lib/python3.9/site-packages (from torch->timm) (11.4.1.48)\n",
      "Requirement already satisfied: nvidia-cusparse-cu11==11.7.5.86 in /home/tejas/miniconda3/envs/pilotnet-torch/lib/python3.9/site-packages (from torch->timm) (11.7.5.86)\n",
      "Requirement already satisfied: nvidia-nccl-cu11==2.19.3 in /home/tejas/miniconda3/envs/pilotnet-torch/lib/python3.9/site-packages (from torch->timm) (2.19.3)\n",
      "Requirement already satisfied: nvidia-nvtx-cu11==11.8.86 in /home/tejas/miniconda3/envs/pilotnet-torch/lib/python3.9/site-packages (from torch->timm) (11.8.86)\n",
      "Requirement already satisfied: triton==2.2.0 in /home/tejas/miniconda3/envs/pilotnet-torch/lib/python3.9/site-packages (from torch->timm) (2.2.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /home/tejas/miniconda3/envs/pilotnet-torch/lib/python3.9/site-packages (from jinja2->torch->timm) (2.1.5)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /home/tejas/miniconda3/envs/pilotnet-torch/lib/python3.9/site-packages (from sympy->torch->timm) (1.3.0)\n",
      "Requirement already satisfied: numpy in /home/tejas/miniconda3/envs/pilotnet-torch/lib/python3.9/site-packages (from torchvision->timm) (1.26.4)\n",
      "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /home/tejas/miniconda3/envs/pilotnet-torch/lib/python3.9/site-packages (from torchvision->timm) (11.3.0)\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m25.1.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.2\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip install efficientnet-pytorch\n",
    "!pip install pytorch-quantization\n",
    "!pip install tensorrt\n",
    "!pip install onnx\n",
    "!pip install timm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "fe7e86c5-0ea0-4370-be47-6c5587c40116",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from torch.quantization import QConfig, default_qconfig\n",
    "from torch.quantization.quantize_fx import prepare_qat_fx, convert_fx\n",
    "import torchvision.transforms as transforms\n",
    "from efficientnet_pytorch import EfficientNet\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Check for GPU availability\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"Using device: {device}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6b34c8a-a8ef-443a-a1a6-97c2c8ab4443",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Choose training option:\n",
      "1. Train FP8 E4M3 only\n",
      "2. Train FP8 E5M2 only\n",
      "3. Compare both FP8 formats\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Enter choice (1-3):  2\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting FP8 EfficientNet Quantization-Aware Training (E5M2 format)\n",
      "Preparing data...\n",
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n",
      "Creating FP8 quantized EfficientNet model (E5M2)...\n",
      "Loaded pretrained weights for efficientnet-b0\n",
      "Replaced 1 layers with FP8 quantized versions\n",
      "Starting training...\n",
      "\n",
      "Epoch 1/30\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:   0%|                                                                      | 1/1563 [00:02<1:11:10,  2.73s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 0, Loss: 2.312988\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:   6%|████▌                                                                 | 101/1563 [00:30<06:52,  3.54it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 100, Loss: 2.267657\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  13%|█████████                                                             | 201/1563 [00:59<06:22,  3.56it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 200, Loss: 2.338567\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  19%|█████████████▍                                                        | 301/1563 [01:27<05:55,  3.55it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 300, Loss: 2.332113\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  26%|█████████████████▉                                                    | 401/1563 [01:55<05:26,  3.55it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 400, Loss: 2.274946\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  32%|██████████████████████▍                                               | 501/1563 [02:23<04:53,  3.61it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 500, Loss: 2.261234\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  38%|██████████████████████████▉                                           | 601/1563 [02:51<04:30,  3.55it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 600, Loss: 2.324489\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  45%|███████████████████████████████▍                                      | 701/1563 [03:20<04:05,  3.51it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 700, Loss: 2.285165\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  51%|███████████████████████████████████▊                                  | 801/1563 [03:48<03:36,  3.51it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 800, Loss: 2.333035\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  58%|████████████████████████████████████████▎                             | 901/1563 [04:16<03:07,  3.53it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 900, Loss: 2.257785\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  64%|████████████████████████████████████████████▏                        | 1001/1563 [04:44<02:38,  3.55it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 1000, Loss: 2.314012\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  70%|████████████████████████████████████████████████▌                    | 1101/1563 [05:12<02:09,  3.56it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 1100, Loss: 2.259036\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  77%|█████████████████████████████████████████████████████                | 1201/1563 [05:41<01:41,  3.56it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 1200, Loss: 2.273772\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  83%|█████████████████████████████████████████████████████████▍           | 1301/1563 [06:09<01:12,  3.61it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 1300, Loss: 2.269225\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  90%|█████████████████████████████████████████████████████████████▊       | 1401/1563 [06:37<00:45,  3.53it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 1400, Loss: 2.194006\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  96%|██████████████████████████████████████████████████████████████████▎  | 1501/1563 [07:05<00:17,  3.52it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 1500, Loss: 2.209489\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|█████████████████████████████████████████████████████████████████████| 1563/1563 [07:23<00:00,  3.53it/s]\n",
      "Validation: 100%|█████████████████████████████████████████████████████████████████████| 313/313 [00:26<00:00, 11.77it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 2.2851, Train Acc: 14.09%\n",
      "Val Loss: 2.2559, Val Acc: 22.27%\n",
      "Best model saved!\n",
      "\n",
      "Epoch 2/30\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:   0%|                                                                        | 1/1563 [00:00<12:55,  2.01it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 0, Loss: 2.330847\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:   6%|████▌                                                                 | 101/1563 [00:28<06:44,  3.62it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 100, Loss: 2.249174\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  13%|█████████                                                             | 201/1563 [00:56<06:22,  3.56it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 200, Loss: 2.262446\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  19%|█████████████▍                                                        | 301/1563 [01:25<06:01,  3.49it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 300, Loss: 2.238708\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  26%|█████████████████▉                                                    | 401/1563 [01:53<05:28,  3.54it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 400, Loss: 2.150638\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  32%|██████████████████████▍                                               | 501/1563 [02:21<05:00,  3.54it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 500, Loss: 2.180843\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  38%|██████████████████████████▉                                           | 601/1563 [02:49<04:32,  3.53it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 600, Loss: 2.207849\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  45%|███████████████████████████████▍                                      | 701/1563 [03:17<04:04,  3.53it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 700, Loss: 2.215028\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  51%|███████████████████████████████████▊                                  | 801/1563 [03:45<03:33,  3.57it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 800, Loss: 2.240824\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  58%|████████████████████████████████████████▎                             | 901/1563 [04:14<03:03,  3.61it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 900, Loss: 2.231720\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  64%|████████████████████████████████████████████▏                        | 1001/1563 [04:42<02:42,  3.46it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 1000, Loss: 2.134579\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  70%|████████████████████████████████████████████████▌                    | 1101/1563 [05:10<02:10,  3.54it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 1100, Loss: 2.217266\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  77%|█████████████████████████████████████████████████████                | 1201/1563 [05:38<01:42,  3.52it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 1200, Loss: 2.230025\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  83%|█████████████████████████████████████████████████████████▍           | 1301/1563 [06:07<01:14,  3.54it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 1300, Loss: 2.247898\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  90%|█████████████████████████████████████████████████████████████▊       | 1401/1563 [06:35<00:45,  3.54it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 1400, Loss: 2.186980\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  96%|██████████████████████████████████████████████████████████████████▎  | 1501/1563 [07:03<00:17,  3.57it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 1500, Loss: 2.205276\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|█████████████████████████████████████████████████████████████████████| 1563/1563 [07:20<00:00,  3.55it/s]\n",
      "Validation: 100%|█████████████████████████████████████████████████████████████████████| 313/313 [00:26<00:00, 11.96it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 2.2200, Train Acc: 21.58%\n",
      "Val Loss: 2.1790, Val Acc: 30.57%\n",
      "Best model saved!\n",
      "\n",
      "Epoch 3/30\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:   0%|                                                                        | 1/1563 [00:00<13:26,  1.94it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 0, Loss: 2.283973\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:   6%|████▌                                                                 | 101/1563 [00:28<06:56,  3.51it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 100, Loss: 2.174902\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  13%|█████████                                                             | 201/1563 [00:56<06:29,  3.49it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 200, Loss: 2.244874\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  19%|█████████████▍                                                        | 301/1563 [01:24<05:52,  3.58it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 300, Loss: 2.124323\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  26%|█████████████████▉                                                    | 401/1563 [01:53<05:25,  3.57it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 400, Loss: 2.176664\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  32%|██████████████████████▍                                               | 501/1563 [02:21<04:53,  3.62it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 500, Loss: 2.126784\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  38%|██████████████████████████▉                                           | 601/1563 [02:49<04:35,  3.49it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 600, Loss: 2.133856\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  45%|███████████████████████████████▍                                      | 701/1563 [03:17<04:04,  3.52it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 700, Loss: 2.162468\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  51%|███████████████████████████████████▊                                  | 801/1563 [03:45<03:34,  3.55it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 800, Loss: 2.196624\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  58%|████████████████████████████████████████▎                             | 901/1563 [04:13<03:06,  3.55it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 900, Loss: 2.159741\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  64%|████████████████████████████████████████████▏                        | 1001/1563 [04:42<02:39,  3.53it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 1000, Loss: 2.203907\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  70%|████████████████████████████████████████████████▌                    | 1101/1563 [05:10<02:09,  3.56it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 1100, Loss: 2.171530\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  77%|█████████████████████████████████████████████████████                | 1201/1563 [05:38<01:42,  3.54it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 1200, Loss: 2.137446\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  83%|█████████████████████████████████████████████████████████▍           | 1301/1563 [06:06<01:12,  3.60it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 1300, Loss: 2.138927\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  90%|█████████████████████████████████████████████████████████████▊       | 1401/1563 [06:35<00:46,  3.50it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 1400, Loss: 2.136591\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  96%|██████████████████████████████████████████████████████████████████▎  | 1501/1563 [07:03<00:17,  3.51it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 1500, Loss: 2.062933\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|█████████████████████████████████████████████████████████████████████| 1563/1563 [07:20<00:00,  3.55it/s]\n",
      "Validation: 100%|█████████████████████████████████████████████████████████████████████| 313/313 [00:26<00:00, 11.81it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 2.1583, Train Acc: 27.67%\n",
      "Val Loss: 2.0620, Val Acc: 37.68%\n",
      "Best model saved!\n",
      "\n",
      "Epoch 4/30\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:   0%|                                                                        | 1/1563 [00:00<12:45,  2.04it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 0, Loss: 2.204725\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:   6%|████▌                                                                 | 101/1563 [00:28<06:45,  3.61it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 100, Loss: 2.155682\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  13%|█████████                                                             | 201/1563 [00:57<06:25,  3.53it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 200, Loss: 2.189961\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  19%|█████████████▍                                                        | 301/1563 [01:25<05:59,  3.51it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 300, Loss: 2.063837\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  26%|█████████████████▉                                                    | 401/1563 [01:53<05:28,  3.54it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 400, Loss: 2.066869\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  32%|██████████████████████▍                                               | 501/1563 [02:21<04:59,  3.55it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 500, Loss: 2.238847\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  38%|██████████████████████████▉                                           | 601/1563 [02:49<04:31,  3.54it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 600, Loss: 2.062911\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  45%|███████████████████████████████▍                                      | 701/1563 [03:18<04:02,  3.55it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 700, Loss: 2.068830\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  51%|███████████████████████████████████▊                                  | 801/1563 [03:46<03:34,  3.56it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 800, Loss: 1.969269\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  58%|████████████████████████████████████████▎                             | 901/1563 [04:14<03:04,  3.58it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 900, Loss: 1.987720\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  64%|████████████████████████████████████████████▏                        | 1001/1563 [04:42<02:41,  3.48it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 1000, Loss: 2.040682\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  70%|████████████████████████████████████████████████▌                    | 1101/1563 [05:11<02:11,  3.51it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 1100, Loss: 2.126520\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  77%|█████████████████████████████████████████████████████                | 1201/1563 [05:39<01:42,  3.53it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 1200, Loss: 2.155226\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  83%|█████████████████████████████████████████████████████████▍           | 1301/1563 [06:07<01:14,  3.52it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 1300, Loss: 2.159193\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  90%|█████████████████████████████████████████████████████████████▊       | 1401/1563 [06:35<00:45,  3.52it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 1400, Loss: 2.130952\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  96%|██████████████████████████████████████████████████████████████████▎  | 1501/1563 [07:04<00:17,  3.53it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 1500, Loss: 2.097268\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|█████████████████████████████████████████████████████████████████████| 1563/1563 [07:21<00:00,  3.54it/s]\n",
      "Validation: 100%|█████████████████████████████████████████████████████████████████████| 313/313 [00:26<00:00, 11.72it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 2.1035, Train Acc: 29.43%\n",
      "Val Loss: 1.9074, Val Acc: 40.94%\n",
      "Best model saved!\n",
      "\n",
      "Epoch 5/30\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:   0%|                                                                        | 1/1563 [00:00<13:06,  1.99it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 0, Loss: 2.022764\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:   6%|████▌                                                                 | 101/1563 [00:28<06:56,  3.51it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 100, Loss: 2.114749\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  13%|█████████                                                             | 201/1563 [00:57<06:24,  3.54it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 200, Loss: 2.089629\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  19%|█████████████▍                                                        | 301/1563 [01:25<05:54,  3.56it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 300, Loss: 2.114773\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  26%|█████████████████▉                                                    | 401/1563 [01:53<05:27,  3.55it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 400, Loss: 1.963186\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  32%|██████████████████████▍                                               | 501/1563 [02:21<04:53,  3.62it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 500, Loss: 2.073832\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  38%|██████████████████████████▉                                           | 601/1563 [02:50<04:31,  3.55it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 600, Loss: 2.230799\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  45%|███████████████████████████████▍                                      | 701/1563 [03:18<04:04,  3.53it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 700, Loss: 2.058305\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  51%|███████████████████████████████████▊                                  | 801/1563 [03:47<03:37,  3.50it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 800, Loss: 2.131987\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  58%|████████████████████████████████████████▎                             | 901/1563 [04:15<03:10,  3.48it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 900, Loss: 2.076921\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  64%|████████████████████████████████████████████▏                        | 1001/1563 [04:43<02:40,  3.49it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 1000, Loss: 2.170782\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  70%|████████████████████████████████████████████████▌                    | 1101/1563 [05:12<02:13,  3.46it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 1100, Loss: 2.053147\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  77%|█████████████████████████████████████████████████████                | 1201/1563 [05:40<01:41,  3.56it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 1200, Loss: 2.287496\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  83%|█████████████████████████████████████████████████████████▍           | 1301/1563 [06:08<01:12,  3.60it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 1300, Loss: 2.122184\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  90%|█████████████████████████████████████████████████████████████▊       | 1401/1563 [06:37<00:45,  3.58it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 1400, Loss: 2.160533\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  96%|██████████████████████████████████████████████████████████████████▎  | 1501/1563 [07:05<00:17,  3.49it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 1500, Loss: 2.074539\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|█████████████████████████████████████████████████████████████████████| 1563/1563 [07:23<00:00,  3.53it/s]\n",
      "Validation: 100%|█████████████████████████████████████████████████████████████████████| 313/313 [00:26<00:00, 11.74it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 2.1104, Train Acc: 26.50%\n",
      "Val Loss: 1.8592, Val Acc: 41.41%\n",
      "Best model saved!\n",
      "\n",
      "Epoch 6/30\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:   0%|                                                                        | 1/1563 [00:00<14:21,  1.81it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 0, Loss: 2.063855\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:   6%|████▌                                                                 | 101/1563 [00:28<06:51,  3.55it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 100, Loss: 2.142179\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  13%|█████████                                                             | 201/1563 [00:57<06:15,  3.63it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 200, Loss: 2.092803\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  19%|█████████████▍                                                        | 301/1563 [01:25<05:58,  3.52it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 300, Loss: 2.132545\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  26%|█████████████████▉                                                    | 401/1563 [01:53<05:33,  3.48it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 400, Loss: 2.116791\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  32%|██████████████████████▍                                               | 501/1563 [02:22<05:04,  3.49it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 500, Loss: 2.127933\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  38%|██████████████████████████▉                                           | 601/1563 [02:50<04:32,  3.52it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 600, Loss: 2.132459\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  45%|███████████████████████████████▍                                      | 701/1563 [03:18<04:03,  3.54it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 700, Loss: 2.122930\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  51%|███████████████████████████████████▊                                  | 801/1563 [03:47<03:36,  3.52it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 800, Loss: 2.119610\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  58%|████████████████████████████████████████▎                             | 901/1563 [04:15<03:05,  3.57it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 900, Loss: 2.122133\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  64%|████████████████████████████████████████████▏                        | 1001/1563 [04:43<02:35,  3.61it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 1000, Loss: 2.071676\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  70%|████████████████████████████████████████████████▌                    | 1101/1563 [05:11<02:12,  3.49it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 1100, Loss: 2.121518\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  77%|█████████████████████████████████████████████████████                | 1201/1563 [05:40<01:45,  3.43it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 1200, Loss: 2.157873\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  83%|█████████████████████████████████████████████████████████▍           | 1301/1563 [06:08<01:14,  3.52it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 1300, Loss: 2.163997\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  90%|█████████████████████████████████████████████████████████████▊       | 1401/1563 [06:36<00:46,  3.52it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 1400, Loss: 2.210775\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  96%|██████████████████████████████████████████████████████████████████▎  | 1501/1563 [07:05<00:17,  3.52it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 1500, Loss: 2.144550\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|█████████████████████████████████████████████████████████████████████| 1563/1563 [07:22<00:00,  3.53it/s]\n",
      "Validation: 100%|█████████████████████████████████████████████████████████████████████| 313/313 [00:26<00:00, 11.72it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 2.1468, Train Acc: 23.20%\n",
      "Val Loss: 1.8768, Val Acc: 39.86%\n",
      "\n",
      "Epoch 7/30\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:   0%|                                                                        | 1/1563 [00:00<13:15,  1.96it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 0, Loss: 2.184054\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:   6%|████▌                                                                 | 101/1563 [00:28<06:57,  3.51it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 100, Loss: 2.204151\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  13%|█████████                                                             | 201/1563 [00:56<06:27,  3.51it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 200, Loss: 2.130811\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  19%|█████████████▍                                                        | 301/1563 [01:25<05:56,  3.54it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 300, Loss: 2.128471\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  26%|█████████████████▉                                                    | 401/1563 [01:53<05:28,  3.54it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 400, Loss: 2.055604\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  32%|██████████████████████▍                                               | 501/1563 [02:21<04:59,  3.54it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 500, Loss: 2.111493\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  38%|██████████████████████████▉                                           | 601/1563 [02:49<04:25,  3.62it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 600, Loss: 2.141406\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  45%|███████████████████████████████▍                                      | 701/1563 [03:17<04:03,  3.54it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 700, Loss: 2.143522\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  51%|███████████████████████████████████▊                                  | 801/1563 [03:46<03:36,  3.53it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 800, Loss: 2.107188\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  58%|████████████████████████████████████████▎                             | 901/1563 [04:14<03:09,  3.49it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 900, Loss: 2.180995\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  64%|████████████████████████████████████████████▏                        | 1001/1563 [04:42<02:40,  3.51it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 1000, Loss: 2.185929\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  70%|████████████████████████████████████████████████▌                    | 1101/1563 [05:10<02:10,  3.55it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 1100, Loss: 2.161424\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  77%|█████████████████████████████████████████████████████                | 1201/1563 [05:38<01:44,  3.48it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 1200, Loss: 2.221120\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  83%|█████████████████████████████████████████████████████████▍           | 1301/1563 [06:07<01:14,  3.50it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 1300, Loss: 2.197825\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  90%|█████████████████████████████████████████████████████████████▊       | 1401/1563 [06:35<00:46,  3.51it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 1400, Loss: 2.198333\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  96%|██████████████████████████████████████████████████████████████████▎  | 1501/1563 [07:03<00:15,  4.08it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 1500, Loss: 2.106270\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|█████████████████████████████████████████████████████████████████████| 1563/1563 [07:21<00:00,  3.54it/s]\n",
      "Validation: 100%|█████████████████████████████████████████████████████████████████████| 313/313 [00:26<00:00, 11.83it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 2.1672, Train Acc: 21.31%\n",
      "Val Loss: 1.8874, Val Acc: 38.47%\n",
      "\n",
      "Epoch 8/30\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:   0%|                                                                        | 1/1563 [00:00<12:12,  2.13it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 0, Loss: 2.297537\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:   6%|████▌                                                                 | 101/1563 [00:28<06:52,  3.54it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 100, Loss: 2.207305\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  13%|█████████                                                             | 201/1563 [00:56<06:17,  3.61it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 200, Loss: 2.180040\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  19%|█████████████▍                                                        | 301/1563 [01:25<05:47,  3.63it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 300, Loss: 2.074892\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  26%|█████████████████▉                                                    | 401/1563 [01:53<05:34,  3.48it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 400, Loss: 2.111446\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  32%|██████████████████████▍                                               | 501/1563 [02:21<05:01,  3.52it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 500, Loss: 2.134717\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  38%|██████████████████████████▉                                           | 601/1563 [02:50<04:32,  3.52it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 600, Loss: 2.219104\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  45%|███████████████████████████████▍                                      | 701/1563 [03:18<04:11,  3.42it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 700, Loss: 2.058351\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  51%|███████████████████████████████████▊                                  | 801/1563 [03:46<03:34,  3.56it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 800, Loss: 2.236920\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  58%|████████████████████████████████████████▎                             | 901/1563 [04:15<03:07,  3.53it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 900, Loss: 2.252281\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  64%|████████████████████████████████████████████▏                        | 1001/1563 [04:43<02:36,  3.59it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 1000, Loss: 2.148509\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  70%|████████████████████████████████████████████████▌                    | 1101/1563 [05:11<02:07,  3.62it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 1100, Loss: 2.011455\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  77%|█████████████████████████████████████████████████████                | 1201/1563 [05:40<01:43,  3.50it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 1200, Loss: 2.154149\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  83%|█████████████████████████████████████████████████████████▍           | 1301/1563 [06:08<01:14,  3.51it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 1300, Loss: 2.191719\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  90%|█████████████████████████████████████████████████████████████▊       | 1401/1563 [06:36<00:46,  3.52it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 1400, Loss: 2.243181\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  96%|██████████████████████████████████████████████████████████████████▎  | 1501/1563 [07:04<00:17,  3.51it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 1500, Loss: 2.135041\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|█████████████████████████████████████████████████████████████████████| 1563/1563 [07:22<00:00,  3.53it/s]\n",
      "Validation: 100%|█████████████████████████████████████████████████████████████████████| 313/313 [00:26<00:00, 11.71it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 2.1740, Train Acc: 20.38%\n",
      "Val Loss: 1.8912, Val Acc: 38.09%\n",
      "\n",
      "Epoch 9/30\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:   0%|                                                                        | 1/1563 [00:00<13:07,  1.98it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 0, Loss: 2.513169\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:   6%|████▌                                                                 | 101/1563 [00:28<06:59,  3.48it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 100, Loss: 2.169976\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  13%|█████████                                                             | 201/1563 [00:57<06:28,  3.50it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 200, Loss: 2.182358\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  19%|█████████████▍                                                        | 301/1563 [01:25<05:59,  3.51it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 300, Loss: 2.127049\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  26%|█████████████████▉                                                    | 401/1563 [01:53<05:31,  3.51it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 400, Loss: 2.226463\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  32%|██████████████████████▍                                               | 501/1563 [02:22<04:59,  3.54it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 500, Loss: 2.043745\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  38%|██████████████████████████▉                                           | 601/1563 [02:50<04:30,  3.56it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 600, Loss: 2.215560\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  45%|███████████████████████████████▍                                      | 701/1563 [03:18<03:59,  3.59it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 700, Loss: 2.192108\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  51%|███████████████████████████████████▊                                  | 801/1563 [03:47<03:37,  3.51it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 800, Loss: 2.278304\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  58%|████████████████████████████████████████▎                             | 901/1563 [04:15<03:08,  3.51it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 900, Loss: 2.263530\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  64%|████████████████████████████████████████████▏                        | 1001/1563 [04:43<02:40,  3.51it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 1000, Loss: 2.119075\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  70%|████████████████████████████████████████████████▌                    | 1101/1563 [05:12<02:11,  3.51it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 1100, Loss: 2.137344\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  77%|█████████████████████████████████████████████████████                | 1201/1563 [05:40<01:42,  3.53it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 1200, Loss: 2.229920\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  83%|█████████████████████████████████████████████████████████▍           | 1301/1563 [06:08<01:13,  3.55it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 1300, Loss: 2.119269\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  90%|█████████████████████████████████████████████████████████████▊       | 1401/1563 [06:37<00:46,  3.52it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 1400, Loss: 2.217913\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  96%|██████████████████████████████████████████████████████████████████▎  | 1501/1563 [07:05<00:17,  3.59it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 1500, Loss: 2.195109\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|█████████████████████████████████████████████████████████████████████| 1563/1563 [07:23<00:00,  3.53it/s]\n",
      "Validation: 100%|█████████████████████████████████████████████████████████████████████| 313/313 [00:26<00:00, 11.76it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 2.1773, Train Acc: 19.96%\n",
      "Val Loss: 1.8951, Val Acc: 37.22%\n",
      "\n",
      "Epoch 10/30\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:   0%|                                                                        | 1/1563 [00:00<13:09,  1.98it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 0, Loss: 2.096561\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:   6%|████▌                                                                 | 101/1563 [00:28<06:54,  3.52it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 100, Loss: 2.235525\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  13%|█████████                                                             | 201/1563 [00:57<06:27,  3.51it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 200, Loss: 2.105760\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  19%|█████████████▍                                                        | 301/1563 [01:25<05:53,  3.57it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 300, Loss: 2.125273\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  26%|█████████████████▉                                                    | 401/1563 [01:54<05:36,  3.45it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 400, Loss: 2.212883\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  32%|██████████████████████▍                                               | 501/1563 [02:22<05:03,  3.50it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 500, Loss: 2.120324\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  38%|██████████████████████████▉                                           | 601/1563 [02:50<04:35,  3.49it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 600, Loss: 2.095806\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  45%|███████████████████████████████▍                                      | 701/1563 [03:18<04:03,  3.54it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 700, Loss: 2.189640\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  51%|███████████████████████████████████▊                                  | 801/1563 [03:47<03:37,  3.50it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 800, Loss: 2.128343\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  58%|████████████████████████████████████████▎                             | 901/1563 [04:15<03:06,  3.54it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 900, Loss: 2.171435\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  64%|████████████████████████████████████████████▏                        | 1001/1563 [04:43<02:41,  3.48it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 1000, Loss: 2.197222\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  70%|████████████████████████████████████████████████▌                    | 1101/1563 [05:12<02:07,  3.62it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 1100, Loss: 2.144023\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  77%|█████████████████████████████████████████████████████                | 1201/1563 [05:40<01:42,  3.52it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 1200, Loss: 2.333473\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  83%|█████████████████████████████████████████████████████████▍           | 1301/1563 [06:08<01:14,  3.51it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 1300, Loss: 2.196064\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  90%|█████████████████████████████████████████████████████████████▊       | 1401/1563 [06:36<00:45,  3.52it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 1400, Loss: 2.171188\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  96%|██████████████████████████████████████████████████████████████████▎  | 1501/1563 [07:05<00:17,  3.51it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 1500, Loss: 2.202278\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|█████████████████████████████████████████████████████████████████████| 1563/1563 [07:22<00:00,  3.53it/s]\n",
      "Validation: 100%|█████████████████████████████████████████████████████████████████████| 313/313 [00:26<00:00, 11.80it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 2.1825, Train Acc: 19.22%\n",
      "Val Loss: 1.9158, Val Acc: 35.96%\n",
      "\n",
      "Epoch 11/30\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:   0%|                                                                        | 1/1563 [00:00<12:59,  2.00it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 0, Loss: 2.328601\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:   6%|████▌                                                                 | 101/1563 [00:28<06:59,  3.49it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 100, Loss: 2.146080\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  13%|█████████                                                             | 201/1563 [00:57<06:27,  3.51it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 200, Loss: 2.263403\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  19%|█████████████▍                                                        | 301/1563 [01:25<06:01,  3.49it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 300, Loss: 2.095564\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  26%|█████████████████▉                                                    | 401/1563 [01:53<05:31,  3.51it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 400, Loss: 2.209829\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  32%|██████████████████████▍                                               | 501/1563 [02:22<05:01,  3.52it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 500, Loss: 2.246247\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  38%|██████████████████████████▉                                           | 601/1563 [02:50<04:32,  3.53it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 600, Loss: 2.151222\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  45%|███████████████████████████████▍                                      | 701/1563 [03:18<04:03,  3.54it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 700, Loss: 2.187951\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  51%|███████████████████████████████████▊                                  | 801/1563 [03:47<03:36,  3.52it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 800, Loss: 2.177580\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  58%|████████████████████████████████████████▎                             | 901/1563 [04:15<03:10,  3.48it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 900, Loss: 2.398126\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  64%|████████████████████████████████████████████▏                        | 1001/1563 [04:43<02:43,  3.43it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 1000, Loss: 2.142757\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  70%|████████████████████████████████████████████████▌                    | 1101/1563 [05:12<02:11,  3.52it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 1100, Loss: 2.089397\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  77%|█████████████████████████████████████████████████████                | 1201/1563 [05:40<01:42,  3.52it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 1200, Loss: 2.104516\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  83%|█████████████████████████████████████████████████████████▍           | 1301/1563 [06:08<01:13,  3.54it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 1300, Loss: 2.073182\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  90%|█████████████████████████████████████████████████████████████▊       | 1401/1563 [06:37<00:45,  3.55it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 1400, Loss: 2.150930\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  96%|██████████████████████████████████████████████████████████████████▎  | 1501/1563 [07:05<00:17,  3.59it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 1500, Loss: 2.255182\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|█████████████████████████████████████████████████████████████████████| 1563/1563 [07:22<00:00,  3.53it/s]\n",
      "Validation: 100%|█████████████████████████████████████████████████████████████████████| 313/313 [00:26<00:00, 11.82it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 2.1846, Train Acc: 18.78%\n",
      "Val Loss: 1.9411, Val Acc: 33.91%\n",
      "\n",
      "Epoch 12/30\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:   0%|                                                                        | 1/1563 [00:00<13:01,  2.00it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 0, Loss: 2.180584\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:   6%|████▌                                                                 | 101/1563 [00:28<06:53,  3.53it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 100, Loss: 2.237802\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  13%|█████████                                                             | 201/1563 [00:56<06:24,  3.55it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 200, Loss: 2.212359\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  19%|█████████████▍                                                        | 301/1563 [01:25<05:55,  3.55it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 300, Loss: 2.147573\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  26%|█████████████████▉                                                    | 401/1563 [01:53<05:23,  3.60it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 400, Loss: 2.234375\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  32%|██████████████████████▍                                               | 501/1563 [02:22<05:02,  3.51it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 500, Loss: 2.145078\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  38%|██████████████████████████▉                                           | 601/1563 [02:50<04:36,  3.48it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 600, Loss: 2.240692\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  45%|███████████████████████████████▍                                      | 701/1563 [03:18<04:10,  3.44it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 700, Loss: 2.099441\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  51%|███████████████████████████████████▊                                  | 801/1563 [03:47<03:36,  3.51it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 800, Loss: 2.245048\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  58%|████████████████████████████████████████▎                             | 901/1563 [04:15<03:08,  3.52it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 900, Loss: 2.152527\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  64%|████████████████████████████████████████████▏                        | 1001/1563 [04:43<02:39,  3.53it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 1000, Loss: 2.188608\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  70%|████████████████████████████████████████████████▌                    | 1101/1563 [05:11<02:08,  3.60it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 1100, Loss: 2.317625\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  77%|█████████████████████████████████████████████████████                | 1201/1563 [05:40<01:40,  3.62it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 1200, Loss: 2.099166\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  83%|█████████████████████████████████████████████████████████▍           | 1301/1563 [06:08<01:15,  3.49it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 1300, Loss: 2.270791\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  90%|█████████████████████████████████████████████████████████████▊       | 1401/1563 [06:36<00:45,  3.53it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 1400, Loss: 2.239974\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  96%|██████████████████████████████████████████████████████████████████▎  | 1501/1563 [07:04<00:17,  3.52it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 1500, Loss: 2.055601\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  96%|██████████████████████████████████████████████████████████████████▎  | 1502/1563 [07:05<00:17,  3.52it/s]"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "\n",
    "import math\n",
    "\n",
    "class FP8Quantizer:\n",
    "    \"\"\"\n",
    "    FP8 E4M3 and E5M2 format quantization implementation\n",
    "    Supports both FP8 formats for optimal performance\n",
    "    \"\"\"\n",
    "    def __init__(self, format_type='E4M3'):\n",
    "        self.format_type = format_type\n",
    "        \n",
    "        if format_type == 'E4M3':\n",
    "            # FP8 E4M3 quantization levels (256 values)\n",
    "            # Range: -448 to 448 with better precision for smaller values\n",
    "            self.max_val = 448.0\n",
    "            self.min_val = -448.0\n",
    "            # Create E4M3 quantization levels\n",
    "            self.fp8_values = self._create_e4m3_values()\n",
    "        else:  # E5M2\n",
    "            # FP8 E5M2 quantization levels (256 values)\n",
    "            # Range: -57344 to 57344 with wider dynamic range\n",
    "            self.max_val = 57344.0\n",
    "            self.min_val = -57344.0\n",
    "            # Create E5M2 quantization levels\n",
    "            self.fp8_values = self._create_e5m2_values()\n",
    "    \n",
    "    def _create_e4m3_values(self):\n",
    "        \"\"\"Create FP8 E4M3 quantization levels\"\"\"\n",
    "        values = []\n",
    "        \n",
    "        # Generate all possible E4M3 values\n",
    "        for i in range(256):\n",
    "            # Extract sign, exponent (4 bits), and mantissa (3 bits)\n",
    "            sign = (i >> 7) & 1\n",
    "            exp = (i >> 3) & 0xF\n",
    "            mant = i & 0x7\n",
    "            \n",
    "            if exp == 0:  # Subnormal numbers\n",
    "                if mant == 0:\n",
    "                    value = 0.0\n",
    "                else:\n",
    "                    value = (-1)**sign * (mant / 8.0) * (2**(-6))\n",
    "            elif exp == 15:  # Special values (NaN, Inf) - treat as max values\n",
    "                if mant == 0:\n",
    "                    value = (-1)**sign * 448.0\n",
    "                else:\n",
    "                    value = float('nan') if sign == 0 else -float('nan')\n",
    "            else:  # Normal numbers\n",
    "                value = (-1)**sign * (1 + mant/8.0) * (2**(exp-7))\n",
    "            \n",
    "            # Filter out NaN values and clamp to valid range\n",
    "            if not math.isnan(value) and not math.isinf(value):\n",
    "                values.append(value)\n",
    "        \n",
    "        # Remove duplicates and sort\n",
    "        values = sorted(list(set(values)))\n",
    "        return torch.tensor(values, dtype=torch.float32)\n",
    "    \n",
    "    def _create_e5m2_values(self):\n",
    "        \"\"\"Create FP8 E5M2 quantization levels\"\"\"\n",
    "        values = []\n",
    "        \n",
    "        # Generate all possible E5M2 values\n",
    "        for i in range(256):\n",
    "            # Extract sign, exponent (5 bits), and mantissa (2 bits)\n",
    "            sign = (i >> 7) & 1\n",
    "            exp = (i >> 2) & 0x1F\n",
    "            mant = i & 0x3\n",
    "            \n",
    "            if exp == 0:  # Subnormal numbers\n",
    "                if mant == 0:\n",
    "                    value = 0.0\n",
    "                else:\n",
    "                    value = (-1)**sign * (mant / 4.0) * (2**(-14))\n",
    "            elif exp == 31:  # Special values (NaN, Inf) - treat as max values\n",
    "                if mant == 0:\n",
    "                    value = (-1)**sign * 57344.0\n",
    "                else:\n",
    "                    value = float('nan') if sign == 0 else -float('nan')\n",
    "            else:  # Normal numbers\n",
    "                value = (-1)**sign * (1 + mant/4.0) * (2**(exp-15))\n",
    "            \n",
    "            # Filter out NaN values and clamp to valid range\n",
    "            if not math.isnan(value) and not math.isinf(value):\n",
    "                values.append(value)\n",
    "        \n",
    "        # Remove duplicates and sort\n",
    "        values = sorted(list(set(values)))\n",
    "        return torch.tensor(values, dtype=torch.float32)\n",
    "    \n",
    "    def quantize(self, x, scale_factor=None):\n",
    "        \"\"\"\n",
    "        Quantize tensor to FP8 format using absmax scaling\n",
    "        \"\"\"\n",
    "        if scale_factor is None:\n",
    "            # Calculate absmax scale factor\n",
    "            max_val = torch.max(torch.abs(x))\n",
    "            scale_factor = self.max_val / (max_val + 1e-8)  # Avoid division by zero\n",
    "        \n",
    "        # Scale input\n",
    "        x_scaled = x * scale_factor\n",
    "        \n",
    "        # Clamp to FP8 range\n",
    "        x_clamped = torch.clamp(x_scaled, self.min_val, self.max_val)\n",
    "        \n",
    "        # Quantize using lookup table\n",
    "        quantized = self._quantize_lookup(x_clamped)\n",
    "        \n",
    "        return quantized / scale_factor, scale_factor\n",
    "    \n",
    "    def _quantize_lookup(self, x):\n",
    "        \"\"\"\n",
    "        Quantize using lookup table for FP8 values\n",
    "        \"\"\"\n",
    "        # Ensure fp8_values are on the same device as x\n",
    "        fp8_values = self.fp8_values.to(x.device)\n",
    "        \n",
    "        # Vectorized quantization using broadcasting\n",
    "        x_expanded = x.unsqueeze(-1)  # [..., 1]\n",
    "        fp8_expanded = fp8_values.unsqueeze(0).expand(*x.shape, -1)  # [..., 256]\n",
    "        \n",
    "        # Find closest FP8 value\n",
    "        diff = torch.abs(x_expanded - fp8_expanded)\n",
    "        closest_indices = torch.argmin(diff, dim=-1)\n",
    "        \n",
    "        return fp8_values[closest_indices]\n",
    "\n",
    "class OutlierClampingCompensation:\n",
    "    \"\"\"\n",
    "    Outlier Clamping and Compensation for activations\n",
    "    Enhanced for FP8 quantization with better outlier handling\n",
    "    \"\"\"\n",
    "    def __init__(self, quantile=0.995):  # Higher quantile for FP8\n",
    "        self.quantile = quantile\n",
    "    \n",
    "    def apply(self, x):\n",
    "        \"\"\"\n",
    "        Apply outlier clamping and return clamped tensor and compensation matrix\n",
    "        \"\"\"\n",
    "        # Calculate quantile thresholds\n",
    "        threshold = torch.quantile(torch.abs(x), self.quantile)\n",
    "        \n",
    "        # Clamp outliers\n",
    "        x_clamped = torch.clamp(x, -threshold, threshold)\n",
    "        \n",
    "        # Calculate compensation matrix (sparse)\n",
    "        compensation = x - x_clamped\n",
    "        \n",
    "        return x_clamped, compensation\n",
    "\n",
    "class QuantizedConv2dFP8(nn.Module):\n",
    "    \"\"\"\n",
    "    Custom quantized convolution with FP8 support\n",
    "    \"\"\"\n",
    "    def __init__(self, in_channels, out_channels, kernel_size, stride=1, padding=0, format_type='E4M3'):\n",
    "        super().__init__()\n",
    "        self.conv = nn.Conv2d(in_channels, out_channels, kernel_size, stride, padding)\n",
    "        self.weight_quantizer = FP8Quantizer(format_type)\n",
    "        self.activation_quantizer = FP8Quantizer(format_type)\n",
    "        self.outlier_handler = OutlierClampingCompensation()\n",
    "        \n",
    "    def forward(self, x):\n",
    "        # Quantize weights\n",
    "        weight_q, weight_scale = self.weight_quantizer.quantize(self.conv.weight)\n",
    "        \n",
    "        # Handle activation outliers\n",
    "        x_clamped, compensation = self.outlier_handler.apply(x)\n",
    "        \n",
    "        # Quantize activations\n",
    "        x_q, act_scale = self.activation_quantizer.quantize(x_clamped)\n",
    "        \n",
    "        # Perform convolution with quantized weights and activations\n",
    "        output = F.conv2d(x_q, weight_q, self.conv.bias, \n",
    "                         self.conv.stride, self.conv.padding)\n",
    "        \n",
    "        # Add compensation for outliers (sparse matrix multiplication)\n",
    "        if torch.sum(torch.abs(compensation)) > 0:\n",
    "            comp_output = F.conv2d(compensation, self.conv.weight, None,\n",
    "                                 self.conv.stride, self.conv.padding)\n",
    "            output += comp_output\n",
    "        \n",
    "        return output\n",
    "\n",
    "class QuantizedLinearFP8(nn.Module):\n",
    "    \"\"\"\n",
    "    Custom quantized linear layer with FP8 support\n",
    "    \"\"\"\n",
    "    def __init__(self, in_features, out_features, format_type='E4M3'):\n",
    "        super().__init__()\n",
    "        self.linear = nn.Linear(in_features, out_features)\n",
    "        self.weight_quantizer = FP8Quantizer(format_type)\n",
    "        self.activation_quantizer = FP8Quantizer(format_type)\n",
    "        self.outlier_handler = OutlierClampingCompensation()\n",
    "        \n",
    "    def forward(self, x):\n",
    "        # Quantize weights\n",
    "        weight_q, weight_scale = self.weight_quantizer.quantize(self.linear.weight)\n",
    "        \n",
    "        # Handle activation outliers\n",
    "        x_clamped, compensation = self.outlier_handler.apply(x)\n",
    "        \n",
    "        # Quantize activations\n",
    "        x_q, act_scale = self.activation_quantizer.quantize(x_clamped)\n",
    "        \n",
    "        # Perform linear operation with quantized weights and activations\n",
    "        output = F.linear(x_q, weight_q, self.linear.bias)\n",
    "        \n",
    "        # Add compensation for outliers\n",
    "        if torch.sum(torch.abs(compensation)) > 0:\n",
    "            comp_output = F.linear(compensation, self.linear.weight, None)\n",
    "            output += comp_output\n",
    "        \n",
    "        return output\n",
    "\n",
    "class QuantizedEfficientNetFP8(nn.Module):\n",
    "    \"\"\"\n",
    "    EfficientNet with FP8 quantization\n",
    "    \"\"\"\n",
    "    def __init__(self, model_name='efficientnet-b0', num_classes=10, format_type='E4M3'):\n",
    "        super().__init__()\n",
    "        # Load pre-trained EfficientNet\n",
    "        self.backbone = EfficientNet.from_pretrained(model_name, num_classes=num_classes)\n",
    "        self.format_type = format_type\n",
    "        \n",
    "        # Replace key layers with quantized versions\n",
    "        self._replace_layers()\n",
    "        \n",
    "    def _replace_layers(self):\n",
    "        \"\"\"\n",
    "        Replace standard layers with FP8 quantized versions\n",
    "        \"\"\"\n",
    "        replaced_count = 0\n",
    "        \n",
    "        # Replace convolutional layers\n",
    "        for name, module in self.backbone.named_modules():\n",
    "            if isinstance(module, nn.Conv2d) and 'features' in name:\n",
    "                # Replace with quantized conv\n",
    "                new_conv = QuantizedConv2dFP8(\n",
    "                    module.in_channels, \n",
    "                    module.out_channels, \n",
    "                    module.kernel_size, \n",
    "                    module.stride, \n",
    "                    module.padding,\n",
    "                    self.format_type\n",
    "                )\n",
    "                # Copy weights\n",
    "                new_conv.conv.weight.data = module.weight.data.clone()\n",
    "                if module.bias is not None:\n",
    "                    new_conv.conv.bias.data = module.bias.data.clone()\n",
    "                \n",
    "                # Replace in model\n",
    "                parent_name = '.'.join(name.split('.')[:-1])\n",
    "                child_name = name.split('.')[-1]\n",
    "                parent = self.backbone\n",
    "                for part in parent_name.split('.'):\n",
    "                    if part:\n",
    "                        parent = getattr(parent, part)\n",
    "                setattr(parent, child_name, new_conv)\n",
    "                replaced_count += 1\n",
    "                \n",
    "        # Replace classifier (linear layer)\n",
    "        if hasattr(self.backbone, '_fc'):\n",
    "            original_fc = self.backbone._fc\n",
    "            new_fc = QuantizedLinearFP8(\n",
    "                original_fc.in_features,\n",
    "                original_fc.out_features,\n",
    "                self.format_type\n",
    "            )\n",
    "            new_fc.linear.weight.data = original_fc.weight.data.clone()\n",
    "            if original_fc.bias is not None:\n",
    "                new_fc.linear.bias.data = original_fc.bias.data.clone()\n",
    "            self.backbone._fc = new_fc\n",
    "            replaced_count += 1\n",
    "            \n",
    "        print(f\"Replaced {replaced_count} layers with FP8 quantized versions\")\n",
    "    \n",
    "    def forward(self, x):\n",
    "        return self.backbone(x)\n",
    "\n",
    "class FP8QATTrainer:\n",
    "    \"\"\"\n",
    "    Quantization-Aware Training trainer for FP8 EfficientNet\n",
    "    \"\"\"\n",
    "    def __init__(self, model, train_loader, val_loader, device):\n",
    "        self.model = model.to(device)\n",
    "        self.train_loader = train_loader\n",
    "        self.val_loader = val_loader\n",
    "        self.device = device\n",
    "        \n",
    "        # Setup optimizer with lower learning rate for QAT\n",
    "        self.optimizer = optim.AdamW(self.model.parameters(), lr=5e-6, weight_decay=1e-4)  # Even lower LR for FP8\n",
    "        self.scheduler = optim.lr_scheduler.CosineAnnealingLR(self.optimizer, T_max=100)\n",
    "        self.criterion = nn.CrossEntropyLoss()\n",
    "        \n",
    "    def train_epoch(self):\n",
    "        \"\"\"\n",
    "        Train for one epoch\n",
    "        \"\"\"\n",
    "        self.model.train()\n",
    "        total_loss = 0\n",
    "        correct = 0\n",
    "        total = 0\n",
    "        \n",
    "        for batch_idx, (data, target) in enumerate(tqdm(self.train_loader, desc=\"Training\")):\n",
    "            data, target = data.to(self.device), target.to(self.device)\n",
    "            \n",
    "            self.optimizer.zero_grad()\n",
    "            output = self.model(data)\n",
    "            loss = self.criterion(output, target)\n",
    "            loss.backward()\n",
    "            \n",
    "            # Gradient clipping for stability (more conservative for FP8)\n",
    "            torch.nn.utils.clip_grad_norm_(self.model.parameters(), max_norm=0.5)\n",
    "            \n",
    "            self.optimizer.step()\n",
    "            \n",
    "            total_loss += loss.item()\n",
    "            pred = output.argmax(dim=1, keepdim=True)\n",
    "            correct += pred.eq(target.view_as(pred)).sum().item()\n",
    "            total += target.size(0)\n",
    "            \n",
    "            if batch_idx % 100 == 0:\n",
    "                print(f'Batch {batch_idx}, Loss: {loss.item():.6f}')\n",
    "        \n",
    "        return total_loss / len(self.train_loader), 100. * correct / total\n",
    "    \n",
    "    def validate(self):\n",
    "        \"\"\"\n",
    "        Validate the model\n",
    "        \"\"\"\n",
    "        self.model.eval()\n",
    "        val_loss = 0\n",
    "        correct = 0\n",
    "        total = 0\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            for data, target in tqdm(self.val_loader, desc=\"Validation\"):\n",
    "                data, target = data.to(self.device), target.to(self.device)\n",
    "                output = self.model(data)\n",
    "                val_loss += self.criterion(output, target).item()\n",
    "                pred = output.argmax(dim=1, keepdim=True)\n",
    "                correct += pred.eq(target.view_as(pred)).sum().item()\n",
    "                total += target.size(0)\n",
    "        \n",
    "        return val_loss / len(self.val_loader), 100. * correct / total\n",
    "    \n",
    "    def train(self, epochs=30):  # Fewer epochs for FP8 as it converges faster\n",
    "        \"\"\"\n",
    "        Full training loop\n",
    "        \"\"\"\n",
    "        train_losses, train_accs = [], []\n",
    "        val_losses, val_accs = [], []\n",
    "        \n",
    "        for epoch in range(epochs):\n",
    "            print(f'\\nEpoch {epoch+1}/{epochs}')\n",
    "            \n",
    "            # Training\n",
    "            train_loss, train_acc = self.train_epoch()\n",
    "            \n",
    "            # Validation\n",
    "            val_loss, val_acc = self.validate()\n",
    "            \n",
    "            # Update scheduler\n",
    "            self.scheduler.step()\n",
    "            \n",
    "            # Store metrics\n",
    "            train_losses.append(train_loss)\n",
    "            train_accs.append(train_acc)\n",
    "            val_losses.append(val_loss)\n",
    "            val_accs.append(val_acc)\n",
    "            \n",
    "            print(f'Train Loss: {train_loss:.4f}, Train Acc: {train_acc:.2f}%')\n",
    "            print(f'Val Loss: {val_loss:.4f}, Val Acc: {val_acc:.2f}%')\n",
    "            \n",
    "            # Save best model\n",
    "            if epoch == 0 or val_acc > max(val_accs[:-1]):\n",
    "                torch.save(self.model.state_dict(), 'best_fp8_efficientnet.pth')\n",
    "                print('Best model saved!')\n",
    "        \n",
    "        return {\n",
    "            'train_losses': train_losses,\n",
    "            'train_accs': train_accs,\n",
    "            'val_losses': val_losses,\n",
    "            'val_accs': val_accs\n",
    "        }\n",
    "\n",
    "def prepare_data(batch_size=32):\n",
    "    \"\"\"\n",
    "    Prepare CIFAR-10 dataset for training\n",
    "    \"\"\"\n",
    "    # Data augmentation for training (enhanced for better FP8 training)\n",
    "    train_transform = transforms.Compose([\n",
    "        transforms.Resize(224),\n",
    "        transforms.RandomHorizontalFlip(),\n",
    "        transforms.RandomRotation(15),  # Slightly more rotation\n",
    "        transforms.ColorJitter(brightness=0.3, contrast=0.3, saturation=0.3, hue=0.1),\n",
    "        transforms.RandomAffine(degrees=0, translate=(0.1, 0.1)),  # Translation augmentation\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "    ])\n",
    "    \n",
    "    # Validation transform\n",
    "    val_transform = transforms.Compose([\n",
    "        transforms.Resize(224),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "    ])\n",
    "    \n",
    "    # Load datasets\n",
    "    from torchvision.datasets import CIFAR10\n",
    "    \n",
    "    train_dataset = CIFAR10(root='./data', train=True, download=True, transform=train_transform)\n",
    "    val_dataset = CIFAR10(root='./data', train=False, download=True, transform=val_transform)\n",
    "    \n",
    "    train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=batch_size, \n",
    "                                             shuffle=True, num_workers=4)\n",
    "    val_loader = torch.utils.data.DataLoader(val_dataset, batch_size=batch_size, \n",
    "                                           shuffle=False, num_workers=4)\n",
    "    \n",
    "    return train_loader, val_loader\n",
    "\n",
    "def main_fp8(format_type='E4M3'):\n",
    "    \"\"\"\n",
    "    Main training pipeline for FP8 EfficientNet\n",
    "    \"\"\"\n",
    "    print(f\"Starting FP8 EfficientNet Quantization-Aware Training ({format_type} format)\")\n",
    "    \n",
    "    # Prepare data\n",
    "    print(\"Preparing data...\")\n",
    "    train_loader, val_loader = prepare_data(batch_size=32)\n",
    "    \n",
    "    # Create quantized model\n",
    "    print(f\"Creating FP8 quantized EfficientNet model ({format_type})...\")\n",
    "    model = QuantizedEfficientNetFP8(model_name='efficientnet-b0', num_classes=10, format_type=format_type)\n",
    "    \n",
    "    # Setup trainer\n",
    "    trainer = FP8QATTrainer(model, train_loader, val_loader, device)\n",
    "    \n",
    "    # Train the model\n",
    "    print(\"Starting training...\")\n",
    "    history = trainer.train(epochs=30)\n",
    "    \n",
    "    # Plot results\n",
    "    plot_training_history(history, format_type)\n",
    "    \n",
    "    print(f\"FP8 {format_type} training completed!\")\n",
    "    return model, history\n",
    "\n",
    "def plot_training_history(history, format_type='FP8'):\n",
    "    \"\"\"\n",
    "    Plot training history\n",
    "    \"\"\"\n",
    "    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(15, 5))\n",
    "    \n",
    "    # Plot loss\n",
    "    ax1.plot(history['train_losses'], label='Train Loss')\n",
    "    ax1.plot(history['val_losses'], label='Val Loss')\n",
    "    ax1.set_title(f'{format_type} Training and Validation Loss')\n",
    "    ax1.set_xlabel('Epoch')\n",
    "    ax1.set_ylabel('Loss')\n",
    "    ax1.legend()\n",
    "    ax1.grid(True)\n",
    "    \n",
    "    # Plot accuracy\n",
    "    ax2.plot(history['train_accs'], label='Train Accuracy')\n",
    "    ax2.plot(history['val_accs'], label='Val Accuracy')\n",
    "    ax2.set_title(f'{format_type} Training and Validation Accuracy')\n",
    "    ax2.set_xlabel('Epoch')\n",
    "    ax2.set_ylabel('Accuracy (%)')\n",
    "    ax2.legend()\n",
    "    ax2.grid(True)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "def evaluate_model_performance(model, test_loader, format_type='FP8'):\n",
    "    \"\"\"\n",
    "    Comprehensive model evaluation\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    class_correct = [0] * 10\n",
    "    class_total = [0] * 10\n",
    "    \n",
    "    # CIFAR-10 class names\n",
    "    classes = ['airplane', 'automobile', 'bird', 'cat', 'deer', \n",
    "               'dog', 'frog', 'horse', 'ship', 'truck']\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for data, target in tqdm(test_loader, desc=f\"Evaluating {format_type}\"):\n",
    "            data, target = data.to(device), target.to(device)\n",
    "            outputs = model(data)\n",
    "            _, predicted = torch.max(outputs, 1)\n",
    "            total += target.size(0)\n",
    "            correct += (predicted == target).sum().item()\n",
    "            \n",
    "            # Per-class accuracy\n",
    "            c = (predicted == target).squeeze()\n",
    "            for i in range(target.size(0)):\n",
    "                label = target[i]\n",
    "                class_correct[label] += c[i].item()\n",
    "                class_total[label] += 1\n",
    "    \n",
    "    # Print results\n",
    "    print(f'\\n{format_type} Overall Accuracy: {100 * correct / total:.2f}%')\n",
    "    print(f'\\n{format_type} Per-class Accuracy:')\n",
    "    for i in range(10):\n",
    "        if class_total[i] > 0:\n",
    "            print(f'{classes[i]}: {100 * class_correct[i] / class_total[i]:.2f}%')\n",
    "\n",
    "def compare_model_sizes_fp8():\n",
    "    \"\"\"\n",
    "    Compare model sizes between FP32 and FP8\n",
    "    \"\"\"\n",
    "    # Original EfficientNet\n",
    "    original_model = EfficientNet.from_pretrained('efficientnet-b0', num_classes=10)\n",
    "    \n",
    "    # Calculate theoretical size reduction\n",
    "    # FP8 uses 8 bits vs FP32's 32 bits = 4x reduction\n",
    "    original_params = sum(p.numel() for p in original_model.parameters())\n",
    "    original_size_mb = original_params * 4 / (1024 * 1024)  # FP32 = 4 bytes per param\n",
    "    fp8_size_mb = original_params * 1 / (1024 * 1024)      # FP8 = 1 byte per param\n",
    "    \n",
    "    print(f\"\\nModel Size Comparison:\")\n",
    "    print(f\"Original (FP32): {original_size_mb:.2f} MB\")\n",
    "    print(f\"FP8 Quantized: {fp8_size_mb:.2f} MB\")\n",
    "    print(f\"Size Reduction: {original_size_mb / fp8_size_mb:.1f}x\")\n",
    "\n",
    "def compare_fp8_formats():\n",
    "    \"\"\"\n",
    "    Compare both FP8 formats (E4M3 vs E5M2)\n",
    "    \"\"\"\n",
    "    print(\"Comparing FP8 E4M3 vs E5M2 formats...\")\n",
    "    \n",
    "    # Prepare data once\n",
    "    train_loader, val_loader = prepare_data(batch_size=32)\n",
    "    \n",
    "    results = {}\n",
    "    \n",
    "    for format_type in ['E4M3', 'E5M2']:\n",
    "        print(f\"\\n{'='*50}\")\n",
    "        print(f\"Training with FP8 {format_type} format\")\n",
    "        print(f\"{'='*50}\")\n",
    "        \n",
    "        # Train model with current format\n",
    "        model, history = main_fp8(format_type)\n",
    "        \n",
    "        # Evaluate model\n",
    "        evaluate_model_performance(model, val_loader, f'FP8-{format_type}')\n",
    "        \n",
    "        # Store results\n",
    "        results[format_type] = {\n",
    "            'model': model,\n",
    "            'history': history,\n",
    "            'final_acc': history['val_accs'][-1]\n",
    "        }\n",
    "    \n",
    "    # Compare results\n",
    "    print(f\"\\n{'='*50}\")\n",
    "    print(\"FP8 Format Comparison Summary\")\n",
    "    print(f\"{'='*50}\")\n",
    "    for format_type, result in results.items():\n",
    "        print(f\"FP8-{format_type} Final Validation Accuracy: {result['final_acc']:.2f}%\")\n",
    "    \n",
    "    return results\n",
    "\n",
    "def export_to_onnx_fp8(model, format_type='E4M3', export_path=None):\n",
    "    \"\"\"\n",
    "    Export FP8 model to ONNX format for deployment\n",
    "    \"\"\"\n",
    "    if export_path is None:\n",
    "        export_path = f'fp8_{format_type.lower()}_efficientnet.onnx'\n",
    "    \n",
    "    model.eval()\n",
    "    dummy_input = torch.randn(1, 3, 224, 224).to(device)\n",
    "    \n",
    "    torch.onnx.export(\n",
    "        model,\n",
    "        dummy_input,\n",
    "        export_path,\n",
    "        input_names=['input'],\n",
    "        output_names=['output'],\n",
    "        dynamic_axes={'input': {0: 'batch_size'}, 'output': {0: 'batch_size'}},\n",
    "        opset_version=13  # Higher opset for better FP8 support\n",
    "    )\n",
    "    print(f\"FP8 {format_type} model exported to {export_path}\")\n",
    "\n",
    "# Run the complete FP8 pipeline\n",
    "if __name__ == \"__main__\":\n",
    "    # Option 1: Train with single format\n",
    "    print(\"Choose training option:\")\n",
    "    print(\"1. Train FP8 E4M3 only\")\n",
    "    print(\"2. Train FP8 E5M2 only\") \n",
    "    print(\"3. Compare both FP8 formats\")\n",
    "    \n",
    "    choice = input(\"Enter choice (1-3): \").strip()\n",
    "    \n",
    "    if choice == '1':\n",
    "        model, history = main_fp8('E4M3')\n",
    "        train_loader, val_loader = prepare_data(batch_size=32)\n",
    "        evaluate_model_performance(model, val_loader, 'FP8-E4M3')\n",
    "        compare_model_sizes_fp8()\n",
    "        export_to_onnx_fp8(model, 'E4M3')\n",
    "        \n",
    "    elif choice == '2':\n",
    "        model, history = main_fp8('E5M2')\n",
    "        train_loader, val_loader = prepare_data(batch_size=32)\n",
    "        evaluate_model_performance(model, val_loader, 'FP8-E5M2')\n",
    "        compare_model_sizes_fp8()\n",
    "        export_to_onnx_fp8(model, 'E5M2')\n",
    "        \n",
    "    elif choice == '3':\n",
    "        results = compare_fp8_formats()\n",
    "        compare_model_sizes_fp8()\n",
    "        \n",
    "        # Export both models\n",
    "        for format_type, result in results.items():\n",
    "            export_to_onnx_fp8(result['model'], format_type)\n",
    "    \n",
    "    else:\n",
    "        print(\"Invalid choice. Running default FP8 E4M3 training...\")\n",
    "        model, history = main_fp8('E4M3')\n",
    "        train_loader, val_loader = prepare_data(batch_size=32)\n",
    "        evaluate_model_performance(model, val_loader, 'FP8-E4M3')\n",
    "        compare_model_sizes_fp8()\n",
    "        export_to_onnx_fp8(model, 'E4M3')\n",
    "\n",
    "# Additional analysis functions\n",
    "def analyze_quantization_error(model, test_loader):\n",
    "    \"\"\"\n",
    "    Analyze quantization error patterns\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "    quantization_errors = []\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for data, _ in test_loader:\n",
    "            data = data.to(device)\n",
    "            \n",
    "            # Get activations from quantized layers\n",
    "            for name, module in model.named_modules():\n",
    "                if isinstance(module, (QuantizedConv2dFP8, QuantizedLinearFP8)):\n",
    "                    # This would require hooks to capture intermediate activations\n",
    "                    # Implementation depends on specific analysis needs\n",
    "                    pass\n",
    "            break  # Analyze only first batch for demonstration\n",
    "    \n",
    "    print(\"Quantization error analysis completed\")\n",
    "\n",
    "def benchmark_inference_speed_fp8(model, test_loader, format_type):\n",
    "    \"\"\"\n",
    "    Benchmark FP8 model inference speed\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "    \n",
    "    # Warmup\n",
    "    with torch.no_grad():\n",
    "        for i, (data, _) in enumerate(test_loader):\n",
    "            if i >= 3:\n",
    "                break\n",
    "            data = data.to(device)\n",
    "            _ = model(data)\n",
    "    \n",
    "    # Benchmark\n",
    "    import time\n",
    "    \n",
    "    torch.cuda.synchronize() if device.type == 'cuda' else None\n",
    "    start_time = time.time()\n",
    "    \n",
    "    num_batches = 10\n",
    "    with torch.no_grad():\n",
    "        for i, (data, _) in enumerate(test_loader):\n",
    "            if i >= num_batches:\n",
    "                break\n",
    "            data = data.to(device)\n",
    "            _ = model(data)\n",
    "    \n",
    "    torch.cuda.synchronize() if device.type == 'cuda' else None\n",
    "    end_time = time.time()\n",
    "    \n",
    "    elapsed_time = end_time - start_time\n",
    "    throughput = (num_batches * test_loader.batch_size) / elapsed_time\n",
    "    \n",
    "    print(f\"FP8 {format_type} Inference Throughput: {throughput:.2f} images/sec\")\n",
    "    return throughput\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1440b1f7-e85a-4e26-9c39-3a8f15ef125a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1000x500 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "# Data\n",
    "models = [\n",
    "    \"VGG-16 CIFAR-10\",\n",
    "    \"ResNet-18 CIFAR-10\",\n",
    "    \"Custom CIFAR-10\",\n",
    "    \"ResNet-18 CIFAR-100\",\n",
    "    \"MobileNetV1 CIFAR-100\",\n",
    "    \"MobileNetV2 CIFAR-100\",\n",
    "    \"ResNet-18 ImageNet\",\n",
    "    \"VIT-B ImageNet\",\n",
    "    \"BERT SST-2\"\n",
    "]\n",
    "\n",
    "fp32 = [91, 91, 94, 75, 64, 73, 64, 83, 90]\n",
    "fxp4 = [90, 90, 74, 73, 63, 72, 63, 78, 77]\n",
    "posit4 = [91.5, 90.5, 82, 74, 63, 73, 63, 81, 82]\n",
    "\n",
    "x = np.arange(len(models))\n",
    "width = 0.25\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(10, 5))\n",
    "ax.bar(x - width, fp32, width, label='FP32', color='orange')\n",
    "ax.bar(x, fxp4, width, label='FxP4', color='dodgerblue')\n",
    "ax.bar(x + width, posit4, width, label='Posit-4', color='green')\n",
    "\n",
    "ax.set_ylabel('Performance (%)')\n",
    "ax.set_title('Model Performance Comparison')\n",
    "ax.set_xticks(x)\n",
    "ax.set_xticklabels(models, rotation=30, ha='right')\n",
    "ax.set_ylim(0, 100)\n",
    "ax.legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9c2efbb-8ff8-41b2-8048-008dfc6abd38",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.23"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
