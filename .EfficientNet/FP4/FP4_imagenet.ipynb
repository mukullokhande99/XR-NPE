{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6fc244ba-7cb0-4cb0-9d9c-f8a55e17482e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install required packages\n",
    "\n",
    "!pip install efficientnet-pytorch\n",
    "!pip install pytorch-quantization\n",
    "!pip install tensorrt\n",
    "!pip install onnx\n",
    "!pip install timm\n",
    "\n",
    "# Import necessary libraries\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from torch.quantization import QConfig, default_qconfig\n",
    "from torch.quantization.quantize_fx import prepare_qat_fx, convert_fx\n",
    "import torchvision.transforms as transforms\n",
    "from efficientnet_pytorch import EfficientNet\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Check for GPU availability\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"Using device: {device}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0070577f-53bd-4024-94d9-a07066dfda7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "\n",
    "class FP4Quantizer:\n",
    "    \"\"\"\n",
    "    FP4 E2M1 format quantization implementation\n",
    "    Based on the format described in the research papers\n",
    "    \"\"\"\n",
    "    def __init__(self, format_type='E2M1'):\n",
    "        self.format_type = format_type\n",
    "        # FP4 E2M1 quantization levels (16 values)\n",
    "        self.fp4_values = torch.tensor([\n",
    "            -6.0, -4.0, -3.0, -2.0, -1.5, -1.0, -0.5, 0.0,\n",
    "            0.5, 1.0, 1.5, 2.0, 3.0, 4.0, 6.0, 0.0  # 16 values\n",
    "        ], dtype=torch.float32)\n",
    "    \n",
    "    def quantize(self, x, scale_factor=None):\n",
    "        \"\"\"\n",
    "        Quantize tensor to FP4 format using absmax scaling\n",
    "        \"\"\"\n",
    "        if scale_factor is None:\n",
    "            # Calculate absmax scale factor\n",
    "            max_val = torch.max(torch.abs(x))\n",
    "            scale_factor = 6.0 / max_val  # 6.0 is MAX_FP4 for E2M1\n",
    "        \n",
    "        # Scale input\n",
    "        x_scaled = x * scale_factor\n",
    "        \n",
    "        # Clamp to FP4 range\n",
    "        x_clamped = torch.clamp(x_scaled, -6.0, 6.0)\n",
    "        \n",
    "        # Quantize using lookup table\n",
    "        quantized = self._quantize_lookup(x_clamped)\n",
    "        \n",
    "        return quantized / scale_factor, scale_factor\n",
    "    \n",
    "    def _quantize_lookup(self, x):\n",
    "        \"\"\"\n",
    "        Quantize using lookup table for FP4 values\n",
    "        \"\"\"\n",
    "        # Find closest FP4 value for each element\n",
    "        x_flat = x.flatten()\n",
    "        quantized_flat = torch.zeros_like(x_flat)\n",
    "        \n",
    "        for i, val in enumerate(x_flat):\n",
    "            diff = torch.abs(self.fp4_values - val)\n",
    "            closest_idx = torch.argmin(diff)\n",
    "            quantized_flat[i] = self.fp4_values[closest_idx]\n",
    "        \n",
    "        return quantized_flat.reshape(x.shape)\n",
    "\n",
    "class OutlierClampingCompensation:\n",
    "    \"\"\"\n",
    "    Outlier Clamping and Compensation for activations\n",
    "    Based on recent FP4 training research\n",
    "    \"\"\"\n",
    "    def __init__(self, quantile=0.99):\n",
    "        self.quantile = quantile\n",
    "    \n",
    "    def apply(self, x):\n",
    "        \"\"\"\n",
    "        Apply outlier clamping and return clamped tensor and compensation matrix\n",
    "        \"\"\"\n",
    "        # Calculate quantile thresholds\n",
    "        threshold = torch.quantile(torch.abs(x), self.quantile)\n",
    "        \n",
    "        # Clamp outliers\n",
    "        x_clamped = torch.clamp(x, -threshold, threshold)\n",
    "        \n",
    "        # Calculate compensation matrix (sparse)\n",
    "        compensation = x - x_clamped\n",
    "        \n",
    "        return x_clamped, compensation\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f85d005f-a715-425c-81aa-5405edecbe79",
   "metadata": {},
   "outputs": [],
   "source": [
    "class QuantizedConv2d(nn.Module):\n",
    "    \"\"\"\n",
    "    Custom quantized convolution with FP4 support\n",
    "    \"\"\"\n",
    "    def __init__(self, in_channels, out_channels, kernel_size, stride=1, padding=0):\n",
    "        super().__init__()\n",
    "        self.conv = nn.Conv2d(in_channels, out_channels, kernel_size, stride, padding)\n",
    "        self.weight_quantizer = FP4Quantizer()\n",
    "        self.activation_quantizer = FP4Quantizer()\n",
    "        self.outlier_handler = OutlierClampingCompensation()\n",
    "        \n",
    "    def forward(self, x):\n",
    "        # Quantize weights\n",
    "        weight_q, weight_scale = self.weight_quantizer.quantize(self.conv.weight)\n",
    "        \n",
    "        # Handle activation outliers\n",
    "        x_clamped, compensation = self.outlier_handler.apply(x)\n",
    "        \n",
    "        # Quantize activations\n",
    "        x_q, act_scale = self.activation_quantizer.quantize(x_clamped)\n",
    "        \n",
    "        # Perform convolution with quantized weights and activations\n",
    "        # Note: This is a simulation - actual hardware would handle this differently\n",
    "        output = F.conv2d(x_q, weight_q, self.conv.bias, \n",
    "                         self.conv.stride, self.conv.padding)\n",
    "        \n",
    "        # Add compensation for outliers (sparse matrix multiplication)\n",
    "        if torch.sum(torch.abs(compensation)) > 0:\n",
    "            comp_output = F.conv2d(compensation, self.conv.weight, None,\n",
    "                                 self.conv.stride, self.conv.padding)\n",
    "            output += comp_output\n",
    "        \n",
    "        return output\n",
    "\n",
    "class QuantizedEfficientNet(nn.Module):\n",
    "    \"\"\"\n",
    "    EfficientNet with FP4 quantization\n",
    "    \"\"\"\n",
    "    def __init__(self, model_name='efficientnet-b0', num_classes=1000):\n",
    "        super().__init__()\n",
    "        # Load pre-trained EfficientNet\n",
    "        self.backbone = EfficientNet.from_pretrained(model_name, num_classes=num_classes)\n",
    "        \n",
    "        # Replace key layers with quantized versions\n",
    "        self._replace_layers()\n",
    "        \n",
    "    def _replace_layers(self):\n",
    "        \"\"\"\n",
    "        Replace standard layers with quantized versions\n",
    "        \"\"\"\n",
    "        # This is a simplified replacement - in practice, you'd need to handle\n",
    "        # all conv layers in the MBConv blocks\n",
    "        for name, module in self.backbone.named_modules():\n",
    "            if isinstance(module, nn.Conv2d) and 'features' in name:\n",
    "                # Replace with quantized conv\n",
    "                new_conv = QuantizedConv2d(\n",
    "                    module.in_channels, \n",
    "                    module.out_channels, \n",
    "                    module.kernel_size, \n",
    "                    module.stride, \n",
    "                    module.padding\n",
    "                )\n",
    "                # Copy weights\n",
    "                new_conv.conv.weight.data = module.weight.data.clone()\n",
    "                if module.bias is not None:\n",
    "                    new_conv.conv.bias.data = module.bias.data.clone()\n",
    "                \n",
    "                # Replace in model\n",
    "                parent_name = '.'.join(name.split('.')[:-1])\n",
    "                child_name = name.split('.')[-1]\n",
    "                parent = self.backbone\n",
    "                for part in parent_name.split('.'):\n",
    "                    if part:\n",
    "                        parent = getattr(parent, part)\n",
    "                setattr(parent, child_name, new_conv)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        return self.backbone(x)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9fbdfe7d-54a2-4d36-9b05-a92e19e43f94",
   "metadata": {},
   "outputs": [],
   "source": [
    "class FP4QATTrainer:\n",
    "    \"\"\"\n",
    "    Quantization-Aware Training trainer for FP4 EfficientNet\n",
    "    \"\"\"\n",
    "    def __init__(self, model, train_loader, val_loader, device):\n",
    "        self.model = model.to(device)\n",
    "        self.train_loader = train_loader\n",
    "        self.val_loader = val_loader\n",
    "        self.device = device\n",
    "        \n",
    "        # Setup optimizer with lower learning rate for QAT\n",
    "        self.optimizer = optim.AdamW(self.model.parameters(), lr=1e-5, weight_decay=1e-4)\n",
    "        self.scheduler = optim.lr_scheduler.CosineAnnealingLR(self.optimizer, T_max=100)\n",
    "        self.criterion = nn.CrossEntropyLoss()\n",
    "        \n",
    "    def train_epoch(self):\n",
    "        \"\"\"\n",
    "        Train for one epoch\n",
    "        \"\"\"\n",
    "        self.model.train()\n",
    "        total_loss = 0\n",
    "        correct = 0\n",
    "        total = 0\n",
    "        \n",
    "        for batch_idx, (data, target) in enumerate(tqdm(self.train_loader, desc=\"Training\")):\n",
    "            data, target = data.to(self.device), target.to(self.device)\n",
    "            \n",
    "            self.optimizer.zero_grad()\n",
    "            output = self.model(data)\n",
    "            loss = self.criterion(output, target)\n",
    "            loss.backward()\n",
    "            \n",
    "            # Gradient clipping for stability\n",
    "            torch.nn.utils.clip_grad_norm_(self.model.parameters(), max_norm=1.0)\n",
    "            \n",
    "            self.optimizer.step()\n",
    "            \n",
    "            total_loss += loss.item()\n",
    "            pred = output.argmax(dim=1, keepdim=True)\n",
    "            correct += pred.eq(target.view_as(pred)).sum().item()\n",
    "            total += target.size(0)\n",
    "            \n",
    "            if batch_idx % 100 == 0:\n",
    "                print(f'Batch {batch_idx}, Loss: {loss.item():.6f}')\n",
    "        \n",
    "        return total_loss / len(self.train_loader), 100. * correct / total\n",
    "    \n",
    "    def validate(self):\n",
    "        \"\"\"\n",
    "        Validate the model\n",
    "        \"\"\"\n",
    "        self.model.eval()\n",
    "        val_loss = 0\n",
    "        correct = 0\n",
    "        total = 0\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            for data, target in tqdm(self.val_loader, desc=\"Validation\"):\n",
    "                data, target = data.to(self.device), target.to(self.device)\n",
    "                output = self.model(data)\n",
    "                val_loss += self.criterion(output, target).item()\n",
    "                pred = output.argmax(dim=1, keepdim=True)\n",
    "                correct += pred.eq(target.view_as(pred)).sum().item()\n",
    "                total += target.size(0)\n",
    "        \n",
    "        return val_loss / len(self.val_loader), 100. * correct / total\n",
    "    \n",
    "    def train(self, epochs=50):\n",
    "        \"\"\"\n",
    "        Full training loop\n",
    "        \"\"\"\n",
    "        train_losses, train_accs = [], []\n",
    "        val_losses, val_accs = [], []\n",
    "        \n",
    "        for epoch in range(epochs):\n",
    "            print(f'\\nEpoch {epoch+1}/{epochs}')\n",
    "            \n",
    "            # Training\n",
    "            train_loss, train_acc = self.train_epoch()\n",
    "            \n",
    "            # Validation\n",
    "            val_loss, val_acc = self.validate()\n",
    "            \n",
    "            # Update scheduler\n",
    "            self.scheduler.step()\n",
    "            \n",
    "            # Store metrics\n",
    "            train_losses.append(train_loss)\n",
    "            train_accs.append(train_acc)\n",
    "            val_losses.append(val_loss)\n",
    "            val_accs.append(val_acc)\n",
    "            \n",
    "            print(f'Train Loss: {train_loss:.4f}, Train Acc: {train_acc:.2f}%')\n",
    "            print(f'Val Loss: {val_loss:.4f}, Val Acc: {val_acc:.2f}%')\n",
    "            \n",
    "            # Save best model\n",
    "            if epoch == 0 or val_acc > max(val_accs[:-1]):\n",
    "                torch.save(self.model.state_dict(), 'best_fp4_efficientnet.pth')\n",
    "                print('Best model saved!')\n",
    "        \n",
    "        return {\n",
    "            'train_losses': train_losses,\n",
    "            'train_accs': train_accs,\n",
    "            'val_losses': val_losses,\n",
    "            'val_accs': val_accs\n",
    "        }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9bd02dad-b7f4-42d2-a753-fac47029339e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_imagenet_data(data_path='/EfficientNet/FP4/Imagenet', batch_size=32):\n",
    "    \"\"\"\n",
    "    Prepare ImageNet dataset for training\n",
    "    \"\"\"\n",
    "    # ImageNet normalization values\n",
    "    normalize = transforms.Normalize(mean=[0.485, 0.456, 0.406], \n",
    "                                   std=[0.229, 0.224, 0.225])\n",
    "    \n",
    "    # Training transforms with progressive resizing support\n",
    "    train_transform = transforms.Compose([\n",
    "        transforms.RandomResizedCrop(224),\n",
    "        transforms.RandomHorizontalFlip(),\n",
    "        transforms.ColorJitter(brightness=0.4, contrast=0.4, saturation=0.4, hue=0.1),\n",
    "        transforms.ToTensor(),\n",
    "        normalize\n",
    "    ])\n",
    "    \n",
    "    # Validation transforms\n",
    "    val_transform = transforms.Compose([\n",
    "        transforms.Resize(256),\n",
    "        transforms.CenterCrop(224),\n",
    "        transforms.ToTensor(),\n",
    "        normalize\n",
    "    ])\n",
    "    \n",
    "    # Load ImageNet dataset\n",
    "    train_dataset = torchvision.datasets.ImageFolder(\n",
    "        os.path.join(data_path, 'train'), \n",
    "        transform=train_transform\n",
    "    )\n",
    "    \n",
    "    val_dataset = torchvision.datasets.ImageFolder(\n",
    "        os.path.join(data_path, 'val'), \n",
    "        transform=val_transform\n",
    "    )\n",
    "    \n",
    "    # Create data loaders\n",
    "    train_loader = torch.utils.data.DataLoader(\n",
    "        train_dataset, \n",
    "        batch_size=batch_size, \n",
    "        shuffle=True, \n",
    "        num_workers=8,\n",
    "        pin_memory=True\n",
    "    )\n",
    "    \n",
    "    val_loader = torch.utils.data.DataLoader(\n",
    "        val_dataset, \n",
    "        batch_size=batch_size, \n",
    "        shuffle=False, \n",
    "        num_workers=8,\n",
    "        pin_memory=True\n",
    "    )\n",
    "    \n",
    "    return train_loader, val_loader\n",
    "\n",
    "# Update model creation for ImageNet (1000 classes)\n",
    "model = QuantizedEfficientNet(model_name='efficientnet-b0', num_classes=1000)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7e0ae62-35fe-4ab1-a8aa-cfde9d6c8d11",
   "metadata": {},
   "outputs": [],
   "source": [
    "def main():\n",
    "    \"\"\"\n",
    "    Main training pipeline for FP4 EfficientNet\n",
    "    \"\"\"\n",
    "    print(\"Starting FP4 EfficientNet Quantization-Aware Training\")\n",
    "    \n",
    "    # Prepare data\n",
    "    print(\"Preparing data...\")\n",
    "    train_loader, val_loader = prepare_data(batch_size=32)\n",
    "    \n",
    "    # Create quantized model\n",
    "    print(\"Creating quantized EfficientNet model...\")\n",
    "    model = QuantizedEfficientNet(model_name='efficientnet-b0', num_classes=10)\n",
    "    \n",
    "    # Setup trainer\n",
    "    trainer = FP4QATTrainer(model, train_loader, val_loader, device)\n",
    "    \n",
    "    # Train the model\n",
    "    print(\"Starting training...\")\n",
    "    history = trainer.train(epochs=50)\n",
    "    \n",
    "    # Plot results\n",
    "    plot_training_history(history)\n",
    "    \n",
    "    print(\"Training completed!\")\n",
    "    return model, history\n",
    "\n",
    "def plot_training_history(history):\n",
    "    \"\"\"\n",
    "    Plot training history\n",
    "    \"\"\"\n",
    "    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(15, 5))\n",
    "    \n",
    "    # Plot loss\n",
    "    ax1.plot(history['train_losses'], label='Train Loss')\n",
    "    ax1.plot(history['val_losses'], label='Val Loss')\n",
    "    ax1.set_title('Training and Validation Loss')\n",
    "    ax1.set_xlabel('Epoch')\n",
    "    ax1.set_ylabel('Loss')\n",
    "    ax1.legend()\n",
    "    ax1.grid(True)\n",
    "    \n",
    "    # Plot accuracy\n",
    "    ax2.plot(history['train_accs'], label='Train Accuracy')\n",
    "    ax2.plot(history['val_accs'], label='Val Accuracy')\n",
    "    ax2.set_title('Training and Validation Accuracy')\n",
    "    ax2.set_xlabel('Epoch')\n",
    "    ax2.set_ylabel('Accuracy (%)')\n",
    "    ax2.legend()\n",
    "    ax2.grid(True)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# Run the complete pipeline\n",
    "if __name__ == \"__main__\":\n",
    "    model, history = main()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
